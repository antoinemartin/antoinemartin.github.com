[{"categories":null,"content":"How to optimize the Terraform destroy process of Azure VMs with extensions and data disk attachments, including practical techniques for managing dependencies and bulk state removals.","date":"05-08","objectID":"/posts/managing-azure-vm-disks-and-extensions-with-terraform-handling-dependency-and-state-removal/","tags":["Terraform","Azure","VM","Disks","Extensions","Infrastructure as Code"],"title":"Managing Azure VM Disks and Extensions with Terraform: Handling Dependency and State Removal","uri":"/posts/managing-azure-vm-disks-and-extensions-with-terraform-handling-dependency-and-state-removal/"},{"categories":null,"content":"When managing Azure Virtual Machines (VMs) with Terraform, especially Windows VMs with extensions and attached data disks, it‚Äôs common to encounter challenges around the order of resource destruction and avoiding unnecessary delays. This post summarizes practical solutions to these issues, with example commands and configuration snippets derived from a real-world scenario. ","date":"05-08","objectID":"/posts/managing-azure-vm-disks-and-extensions-with-terraform-handling-dependency-and-state-removal/:0:0","tags":["Terraform","Azure","VM","Disks","Extensions","Infrastructure as Code"],"title":"Managing Azure VM Disks and Extensions with Terraform: Handling Dependency and State Removal","uri":"/posts/managing-azure-vm-disks-and-extensions-with-terraform-handling-dependency-and-state-removal/"},{"categories":null,"content":"The Problem: Terraform Deletes Extensions and Disks Before the VM By default, Terraform tracks each Azure VM extension (azurerm_virtual_machine_extension) and each data disk attachment (azurerm_virtual_machine_data_disk_attachment) as independent resources. When running terraform destroy, Terraform deletes these extensions and disk attachments one by one before deleting the VM. This sequence can slow down the destroy process and sometimes cause failures if the VM or disks are not in the expected state. In Azure itself, deleting a VM automatically deletes extensions attached to it, making explicit extension destruction redundant. Similarly, Terraform attempts to delete data disk attachments before the VM, which would cause the disk to be actually unattached from the running VM, with possible unexpected results. ","date":"05-08","objectID":"/posts/managing-azure-vm-disks-and-extensions-with-terraform-handling-dependency-and-state-removal/:1:0","tags":["Terraform","Azure","VM","Disks","Extensions","Infrastructure as Code"],"title":"Managing Azure VM Disks and Extensions with Terraform: Handling Dependency and State Removal","uri":"/posts/managing-azure-vm-disks-and-extensions-with-terraform-handling-dependency-and-state-removal/"},{"categories":null,"content":"Solution Overview: Managing Dependencies and Terraform State ","date":"05-08","objectID":"/posts/managing-azure-vm-disks-and-extensions-with-terraform-handling-dependency-and-state-removal/:2:0","tags":["Terraform","Azure","VM","Disks","Extensions","Infrastructure as Code"],"title":"Managing Azure VM Disks and Extensions with Terraform: Handling Dependency and State Removal","uri":"/posts/managing-azure-vm-disks-and-extensions-with-terraform-handling-dependency-and-state-removal/"},{"categories":null,"content":"1. Remove Extensions and Disk Attachments from Terraform State To avoid Terraform explicitly deleting extensions and attachments, you can remove these resources from Terraform state, so Terraform forgets about them and lets Azure handle deletion cascades when the VM is deleted. Use shell commands like these to bulk-remove all extensions or data disk attachments from your state: # Remove all VM extensions from state terraform state rm $(terraform state list | grep azurerm_virtual_machine_extension) # Remove all data disk attachments from state terraform state rm $(terraform state list | grep azurerm_virtual_machine_data_disk_attachment) Note: Always back up your Terraform state file before performing bulk state removals. Backup \u0026 restore the terraform state Before performing any bulk state operations, it‚Äôs crucial to create a backup of your Terraform state file. Here is the recommended approach: # Pull current state to a local backup file terraform state pull \u003e terraform.tfstate.backup.$(date +%Y%m%d_%H%M%S) # Verify the backup contains your resources terraform show terraform.tfstate.backup.$(date +%Y%m%d_%H%M%S) To restore from backup if needed: # For remote state, push the backup back terraform state push terraform.tfstate.backup.20250105_143000 ","date":"05-08","objectID":"/posts/managing-azure-vm-disks-and-extensions-with-terraform-handling-dependency-and-state-removal/:2:1","tags":["Terraform","Azure","VM","Disks","Extensions","Infrastructure as Code"],"title":"Managing Azure VM Disks and Extensions with Terraform: Handling Dependency and State Removal","uri":"/posts/managing-azure-vm-disks-and-extensions-with-terraform-handling-dependency-and-state-removal/"},{"categories":null,"content":"2. Enforce Disk-VM Destroy Ordering Using depends_on Simply removing disk attachments from state can cause Terraform to try deleting disks before the VM, which fails if disks are still attached. The reliable fix is to add explicit dependencies so Terraform destroys resources in the correct order. For example, in your VM resource definition, add a depends_on block referencing the disks, ensuring Terraform deletes disks after the VM: resource \"azurerm_windows_virtual_machine\" \"vm\" { # VM configuration... depends_on = [ azurerm_managed_disk.additionnal_disks, # add all relevant disk attachment resources here ] } This setup tells Terraform: wait until those disk attachments are destroyed before deleting the VM, avoiding the race condition that causes failure. ","date":"05-08","objectID":"/posts/managing-azure-vm-disks-and-extensions-with-terraform-handling-dependency-and-state-removal/:2:2","tags":["Terraform","Azure","VM","Disks","Extensions","Infrastructure as Code"],"title":"Managing Azure VM Disks and Extensions with Terraform: Handling Dependency and State Removal","uri":"/posts/managing-azure-vm-disks-and-extensions-with-terraform-handling-dependency-and-state-removal/"},{"categories":null,"content":"Why Explicit Dependencies? Terraform often establishes implicit dependencies through references, but these sometimes represent only ‚Äúresource existence,‚Äù not full ‚Äúreadiness.‚Äù Azure VM extensions and disk attachments require the VM to be fully provisioned (or deprovisioned) before related resources can be managed safely. The depends_on attribute forces Terraform to respect this ordering. ","date":"05-08","objectID":"/posts/managing-azure-vm-disks-and-extensions-with-terraform-handling-dependency-and-state-removal/:2:3","tags":["Terraform","Azure","VM","Disks","Extensions","Infrastructure as Code"],"title":"Managing Azure VM Disks and Extensions with Terraform: Handling Dependency and State Removal","uri":"/posts/managing-azure-vm-disks-and-extensions-with-terraform-handling-dependency-and-state-removal/"},{"categories":null,"content":"Summary Table: Terraform Destroy Behavior for Azure VM Components Resource Type Default Terraform Behavior Recommended Action VM Extensions Destroyed first, then VM Remove from state before VM destroy, or leave as is if okay Data Disk Attachments Destroyed before VM Use depends_on in VM to depend on disk attachments Managed Disks Destroyed after attachments Generally safe with proper dependencies VM Destroy last Ensure all dependencies cleared or ordered correctly ","date":"05-08","objectID":"/posts/managing-azure-vm-disks-and-extensions-with-terraform-handling-dependency-and-state-removal/:3:0","tags":["Terraform","Azure","VM","Disks","Extensions","Infrastructure as Code"],"title":"Managing Azure VM Disks and Extensions with Terraform: Handling Dependency and State Removal","uri":"/posts/managing-azure-vm-disks-and-extensions-with-terraform-handling-dependency-and-state-removal/"},{"categories":null,"content":"Final Recommendations When destroying VMs with extensions, remove extensions from Terraform state to speed up destroy and rely on Azure‚Äôs automatic cleanup. For disks, do not remove disk attachments or disks from state blindly. Instead, add explicit depends_on dependencies in the VM resource to ensure Terraform respects the correct destroy order. Always backup your Terraform state before any bulk operations. For quick environment cleanup, consider deleting the whole resource group which cascades deletes in Azure. Terraform allows robust, repeatable management of complex Azure infrastructures, but some manual steps and explicit dependency handling are necessary to optimize operations involving VMs with extensions and attached data disks. The approach described here ‚Äî combining state resource removals and dependency declarations ‚Äî has proven reliable in practice for ensuring clean, efficient VM destruction workflows. ","date":"05-08","objectID":"/posts/managing-azure-vm-disks-and-extensions-with-terraform-handling-dependency-and-state-removal/:4:0","tags":["Terraform","Azure","VM","Disks","Extensions","Infrastructure as Code"],"title":"Managing Azure VM Disks and Extensions with Terraform: Handling Dependency and State Removal","uri":"/posts/managing-azure-vm-disks-and-extensions-with-terraform-handling-dependency-and-state-removal/"},{"categories":null,"content":"Complete Example for Explicit Disk Dependency resource \"azurerm_virtual_machine_data_disk_attachment\" \"data_disk_1\" { managed_disk_id = azurerm_managed_disk.data_disk_1.id virtual_machine_id = azurerm_windows_virtual_machine.vm.id lun = 0 caching = \"ReadOnly\" } resource \"azurerm_windows_virtual_machine\" \"vm\" { # VM configuration... depends_on = [ azurerm_virtual_machine_data_disk_attachment.data_disk_1 ] } Using these techniques will help you avoid errors during destruction and save time by preventing Terraform from destroying extensions and disk attachments needlessly. If you have questions or want to share more tips on Terraform with Azure, feel free to comment below! ","date":"05-08","objectID":"/posts/managing-azure-vm-disks-and-extensions-with-terraform-handling-dependency-and-state-removal/:4:1","tags":["Terraform","Azure","VM","Disks","Extensions","Infrastructure as Code"],"title":"Managing Azure VM Disks and Extensions with Terraform: Handling Dependency and State Removal","uri":"/posts/managing-azure-vm-disks-and-extensions-with-terraform-handling-dependency-and-state-removal/"},{"categories":null,"content":"References and Further Reading For additional information on the topics covered in this post, consider these resources: Terraform Azure Provider Documentation Virtual Machine Data Disk Attachment Resource - Official documentation for managing disk attachments Azure Terraform Developer Guide - Microsoft‚Äôs comprehensive guide to using Terraform with Azure Dependency Management Best Practices Mastering Dependencies in Azure Terraform - In-depth guide on handling complex resource dependencies Understanding Terraform depends_on - Comprehensive explanation of explicit dependency management Community Solutions and Examples Azure VM Module with Managed Disks - Production-ready example from Azure‚Äôs official VM module Data Disk Configuration Discussion - Community solutions for VM provisioning with data disks Advanced Topics Terraform Azure Disk Encryption Module - For implementing disk encryption at rest Terraform Azure VM Dependencies Tutorial - Video walkthrough of dependency management Known Issues and Workarounds Azure Provider Issue #28887 - Tracking ongoing improvements to VM resource management ","date":"05-08","objectID":"/posts/managing-azure-vm-disks-and-extensions-with-terraform-handling-dependency-and-state-removal/:5:0","tags":["Terraform","Azure","VM","Disks","Extensions","Infrastructure as Code"],"title":"Managing Azure VM Disks and Extensions with Terraform: Handling Dependency and State Removal","uri":"/posts/managing-azure-vm-disks-and-extensions-with-terraform-handling-dependency-and-state-removal/"},{"categories":null,"content":" I‚Äôm a hands-on technical leader with deep expertise in mobile apps, web front-end and backend services. Back in the 90s, I helped pioneer the French media web with Lib√©ration, Le Monde, and France T√©l√©visions. Got into mobile development in 2001, way before iPhones were cool. Started with embedded navigation and mobile imaging, then got my hands dirty with pretty much every platform out there‚Äîfrom proprietary handset OSes to iOS and Android, plus all the fun stuff like Windows Mobile, J2ME, Symbian, Doja, and even Qualcomm‚Äôs Brew. On the web frontend side, I‚Äôve seen it all evolve from static HTML to today‚Äôs reactive frameworks, going through the whole templating era‚ÄîJSP, PHP, ASP, Django templates, and finally landing on React, Angular, Vue and the modern JavaScript ecosystem. On the server side, I‚Äôve ridden the wave from basic CGI to modern GRPC and WebSockets, picking up everything along the way: Perl, PHP, J2EE, Rails, Django, FastAPI, NestJS, GraphQL‚Ä¶ You name it. I‚Äôve literally mounted servers in racks and built cloud infrastructure with code. Deployed on all the major clouds‚ÄîAWS, Azure and GCP and french ones OVHCloud (OpenStack) and Scaleway. I‚Äôve worked everywhere from startups to Fortune 500s in utilities, media, and tech. I eventually served as VP Engineering and CTO but never stopped coding (because honestly, it‚Äôs still fun). Entrepreneur, Team builder and technical mentor, I love to cultivate software ecosystems and make things happen. Follow this link to see my resume (pdf). Also available in french (pdf) and Spanish (pdf). ","date":"30-07","objectID":"/about/:0:0","tags":null,"title":"About me","uri":"/about/"},{"categories":["Devops"],"content":"How to debug locally on a windows machine an OpenStack image\n","date":"13-01","objectID":"/posts/2023/01/13/debugging-a-failing-openstack-image/","tags":["Openstack","cloud-init","hyper-v","wsl","windows"],"title":"Debugging a failing OpenStack image","uri":"/posts/2023/01/13/debugging-a-failing-openstack-image/"},{"categories":["Devops"],"content":"The problem On my alpine-openstack-vm project, There is a CI process producing a VM image for OpenStack. The process involves testing that the machine boots. The test fails, but the machine is actually booted. What doesn‚Äôt work is the ssh access. As the machine can only be reached via SSH with a private key for obvious security reasons, not having access prevents proper debug. ","date":"13-01","objectID":"/posts/2023/01/13/debugging-a-failing-openstack-image/:1:0","tags":["Openstack","cloud-init","hyper-v","wsl","windows"],"title":"Debugging a failing OpenStack image","uri":"/posts/2023/01/13/debugging-a-failing-openstack-image/"},{"categories":["Devops"],"content":"The objective The objective is to be able to run the produced VM locally to assess the issue. As the image in its current form doesn‚Äôt work, the VM image needs to be slightly modified in order to allow access. If the VM is run locally, adding a root password should be sufficient. ","date":"13-01","objectID":"/posts/2023/01/13/debugging-a-failing-openstack-image/:2:0","tags":["Openstack","cloud-init","hyper-v","wsl","windows"],"title":"Debugging a failing OpenStack image","uri":"/posts/2023/01/13/debugging-a-failing-openstack-image/"},{"categories":["Devops"],"content":"The tools Windows 11 Hyper-V. Information on installation is here. If you are using WSL, you should already be set up or really close. PowerShell. A Windows Subsystem for Linux (WSL) distribution with qemu-img installed. You can set it up faster with Powershell-WSL-Manager. We need to download your Openstack image locally. Hopefully, it is kept as an artifact of our build. ","date":"13-01","objectID":"/posts/2023/01/13/debugging-a-failing-openstack-image/:3:0","tags":["Openstack","cloud-init","hyper-v","wsl","windows"],"title":"Debugging a failing OpenStack image","uri":"/posts/2023/01/13/debugging-a-failing-openstack-image/"},{"categories":["Devops"],"content":"The steps ","date":"13-01","objectID":"/posts/2023/01/13/debugging-a-failing-openstack-image/:4:0","tags":["Openstack","cloud-init","hyper-v","wsl","windows"],"title":"Debugging a failing OpenStack image","uri":"/posts/2023/01/13/debugging-a-failing-openstack-image/"},{"categories":["Devops"],"content":"Image conversion We are going to convert the image from the qcow2 format to the vhdx in order to be able to use it as the base disk of an Hyper-V VM. This will also allow us to mount it inside a WSL distribution to modify it. This is done with the qemu-img command: ‚ùØ qemu-img convert alpine-openstack.qcow2 -O vhdx -o subformat=dynamic alpine-openstack.vhdx ","date":"13-01","objectID":"/posts/2023/01/13/debugging-a-failing-openstack-image/:4:1","tags":["Openstack","cloud-init","hyper-v","wsl","windows"],"title":"Debugging a failing OpenStack image","uri":"/posts/2023/01/13/debugging-a-failing-openstack-image/"},{"categories":["Devops"],"content":"Mounting the image Now we can mount the image in WSL: ‚ùØ wsl.exe --mount --vhd alpine-openstack.vhdx Le disque a √©t√© mont√© en tant que ¬´ /mnt/wsl/CUsersAntoineMartinDownloadsalpineopenstackvhdx ¬ª. Remarque : l‚Äôemplacement sera diff√©rent si vous avez modifi√© le param√®tre automount.root dans /etc/wsl.conf. Pour d√©monter et d√©tacher le disque, ex√©cutez ¬´ wsl.exe --unmount \\\\?\\C:\\Users\\AntoineMartin\\Downloads\\alpine-openstack.vhdx ¬ª. ‚ùØ ","date":"13-01","objectID":"/posts/2023/01/13/debugging-a-failing-openstack-image/:4:2","tags":["Openstack","cloud-init","hyper-v","wsl","windows"],"title":"Debugging a failing OpenStack image","uri":"/posts/2023/01/13/debugging-a-failing-openstack-image/"},{"categories":["Devops"],"content":"Chrooting inside the image We can now chroot inside the image to modify it: ‚ùØ chroot /mnt/wsl/CUsersAntoineMartinDownloadsalpineopenstackvhdx ‚ûú / ","date":"13-01","objectID":"/posts/2023/01/13/debugging-a-failing-openstack-image/:4:3","tags":["Openstack","cloud-init","hyper-v","wsl","windows"],"title":"Debugging a failing OpenStack image","uri":"/posts/2023/01/13/debugging-a-failing-openstack-image/"},{"categories":["Devops"],"content":"Allowing access to the image We set a root password in order to be able to login in our VM: ‚ûú ~ passwd New password: Retype new password: passwd: password updated successfully ‚ûú ~ in /etc/ssh/sshd_config, we enable root login and password authentication: # To disable tunneled clear text passwords, change to no here! PasswordAuthentication yes PermitRootLogin yes In /etc/cloud/cloud.cfg, we allow ssh password authentication: ssh_pwauth: true At this point, you can also add a /root/.ssh/authorized_keys file containing a public key for ssh authenticiation. Then you can exit the chroot with the exit command. ","date":"13-01","objectID":"/posts/2023/01/13/debugging-a-failing-openstack-image/:4:4","tags":["Openstack","cloud-init","hyper-v","wsl","windows"],"title":"Debugging a failing OpenStack image","uri":"/posts/2023/01/13/debugging-a-failing-openstack-image/"},{"categories":["Devops"],"content":"Unmounting the image From windows, type: PS\u003e wsl --unmount alpine-openstack.vhdx ","date":"13-01","objectID":"/posts/2023/01/13/debugging-a-failing-openstack-image/:4:5","tags":["Openstack","cloud-init","hyper-v","wsl","windows"],"title":"Debugging a failing OpenStack image","uri":"/posts/2023/01/13/debugging-a-failing-openstack-image/"},{"categories":["Devops"],"content":"Creating the Hyper-V VM Warning To be able to create Hyper-V VMs from the command line, your user needs to be part of the Hyper-V Administrators group. You can add your user to this group with the command line: PS\u003e # In a `Run as Administrator` terminal (consider sudo from scoop) PS\u003e net localgroup \"Hyper-V Administrators\" \u003cUserName\u003e /add Unfortunately, you will also need to restart your machine üòû Create and start the VM from the command line: # Isolating the VM PS\u003e mkdir debug PS\u003e mv alpine-openstack.vhdx debug PS\u003e cd debug # Creating the VM PS\u003e New-VM -Name debug -MemoryStartupBytes 2GB -Path . -BootDevice VHD -VHDPath .\\alpine-openstack.vhdx -SwitchName \"Default Switch\" -Generation 1 Name State CPUUsage(%) MemoryAssigned(M) Uptime Status Version ---- ----- ----------- ----------------- ------ ------ ------- debug Off 0 0 00:00:00 Fonctionnement normal 11.0 PS\u003e Start-VM -Name debug Tip We use -Generation 1 here in order to avoid UEFI initialization. Now you can connect to the VM: PS\u003e vmconnect localhost debug And you should be able to see the VM window: The actual issue appears clearly in the image above. You should be able to connect as root with the password you entered before: And look at the issues that created the error. You can also connect with ssh. To find the IP address of the machine, you can type the following command on Powershell: PS\u003e Get-NetNeighbor -LinkLayerAddress 00-15-5d-* ifIndex IPAddress LinkLayerAddress State PolicyStore ------- --------- ---------------- ----- ----------- 60 172.26.137.156 00-15-5D-25-05-1D Reachable ActiveStore 29 172.20.68.230 00-15-5D-00-3F-24 Permanent ActiveStore Here the address of the machine would be the second one, the one marked with Permanent. You can connect to it: PS\u003e ssh root@172.20.68.230 Warning: Permanently added '172.20.68.230' (ED25519) to the list of known hosts. root@172.20.68.230's password: Welcome to Alpine! The Alpine Wiki contains a large amount of how-to guides and general information about administrating Alpine systems. See \u003chttps://wiki.alpinelinux.org/\u003e. You can setup the system with the command: setup-alpine You may change this message by editing /etc/motd. ‚ûú ~ On the terminal window as well as with ssh, you are now able to find the issue with your image. Once the reason of the issue is found, you can delete the VM: PS\u003e Stop-VM debug PS\u003e Remove-VM debug -Force PS\u003e ","date":"13-01","objectID":"/posts/2023/01/13/debugging-a-failing-openstack-image/:4:6","tags":["Openstack","cloud-init","hyper-v","wsl","windows"],"title":"Debugging a failing OpenStack image","uri":"/posts/2023/01/13/debugging-a-failing-openstack-image/"},{"categories":["Devops"],"content":"Final thoughts The above procedure is interesting not only for debugging non working images but also to run any image as long as the image format can be handled by qemu. Having WSL at hand is also handy to mount and hack the image. ","date":"13-01","objectID":"/posts/2023/01/13/debugging-a-failing-openstack-image/:5:0","tags":["Openstack","cloud-init","hyper-v","wsl","windows"],"title":"Debugging a failing OpenStack image","uri":"/posts/2023/01/13/debugging-a-failing-openstack-image/"},{"categories":["Development"],"content":"WSL rocks to manage docker instantiations\n","date":"11-05","objectID":"/posts/2022/05/11/multiple-docker-environments-on-windows/","tags":["windows","docker","wsl","powershell"],"title":"Multiple Docker environments on Windows","uri":"/posts/2022/05/11/multiple-docker-environments-on-windows/"},{"categories":["Development"],"content":"On Windows and docker platforms, your docker environment tends to get messy as time goes by. You can start over from a clean sheet with docker system prune --all but sometimes you would like to keep some images around. This post shows how to set up multiple docker environments on Windows with the help of WSL2 (Windows Subsystem For Linux) and Alpine. ","date":"11-05","objectID":"/posts/2022/05/11/multiple-docker-environments-on-windows/:0:0","tags":["windows","docker","wsl","powershell"],"title":"Multiple Docker environments on Windows","uri":"/posts/2022/05/11/multiple-docker-environments-on-windows/"},{"categories":["Development"],"content":"How it works The idea is to run docker on WSL and use a docker alias on the windows side to invoke the docker command in the WSL distribution. This is a well-known usage pattern with or without Docker desktop. With this scenario, nothing prevents us having several WSL distributions with docker. To the contrary, we are going to have several of them, and depending on the environment we want to target, the actual docker command will run on one distribution or the other. To avoid the manual configuration of the several distributions, we are going to use the Wsl-Alpine powershell module I have created (github here). It will allow us create a distribution template we can reuse for each docker environment. ","date":"11-05","objectID":"/posts/2022/05/11/multiple-docker-environments-on-windows/:1:0","tags":["windows","docker","wsl","powershell"],"title":"Multiple Docker environments on Windows","uri":"/posts/2022/05/11/multiple-docker-environments-on-windows/"},{"categories":["Development"],"content":"Pre-requisites In order to perform this guide, you will need: To have a working WSL environment. Instructions are here. To be able to install powershell modules from the Powershell Gallery. You may need to update PowershellGet on your system. ","date":"11-05","objectID":"/posts/2022/05/11/multiple-docker-environments-on-windows/:2:0","tags":["windows","docker","wsl","powershell"],"title":"Multiple Docker environments on Windows","uri":"/posts/2022/05/11/multiple-docker-environments-on-windows/"},{"categories":["Development"],"content":"Installing the Wsl-Alpine module Install the Wsl-Alpine module with the following command: PS\u003e Install-Module -Name Wsl-Alpine ","date":"11-05","objectID":"/posts/2022/05/11/multiple-docker-environments-on-windows/:3:0","tags":["windows","docker","wsl","powershell"],"title":"Multiple Docker environments on Windows","uri":"/posts/2022/05/11/multiple-docker-environments-on-windows/"},{"categories":["Development"],"content":"Creating the docker environment In this step, we are going to create a WSL distribution template to run docker. We will then use this template for our different docker environments. We will start by creating a base Alpine WSL distribution: PS‚ùØ install-WslAlpine docker ####\u003e Creating directory [C:\\Users\\AntoineMartin\\AppData\\Local\\docker]... ####\u003e Downloading https://dl-cdn.alpinelinux.org/alpine/v3.15/releases/x86_64/alpine-minirootfs-3.15.0-x86_64.tar.gz √¢‚Ä†‚Äô C:\\Users\\AntoineMartin\\AppData\\Local\\docker\\rootfs.tar.gz... ####\u003e Creating distribution [docker]... ####\u003e Running initialization script on distribution [docker]... ####\u003e Done. Command to enter distribution: wsl -d docker PS‚ùØ We will then configure the distribution for docker: # Get into to the distribution PS‚ùØ wsl -d docker -u root [powerlevel10k] fetching gitstatusd .. [ok] # Add docker ‚ùØ apk --update add docker (1/13) Installing libseccomp (2.5.2-r0) ... OK: 304 MiB in 103 packages # Start docker with OpenRC ‚ùØ rc-update add docker default * service docker added to runlevel default # Enable OpenRC (will start docker) ‚ùØ openrc default * Caching service dependencies ... [ ok ] * /var/log/docker.log: creating file * /var/log/docker.log: correcting owner * Starting Docker Daemon ... [ ok ] # Allow default user to run docker ‚ùØ addgroup alpine docker # Return to powershell ‚ùØ exit PS‚ùØ Then we can test the distrbitution can run docker: PS‚ùØ wsl -d docker docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES And we can stop it: # Terminate distrbution PS‚ùØ wsl --terminate docker ","date":"11-05","objectID":"/posts/2022/05/11/multiple-docker-environments-on-windows/:4:0","tags":["windows","docker","wsl","powershell"],"title":"Multiple Docker environments on Windows","uri":"/posts/2022/05/11/multiple-docker-environments-on-windows/"},{"categories":["Development"],"content":"Saving the docker distribution for reuse Now we are going to save the WSL distribution in its current state in order to be able to reuse it. PS\u003e Export-WslAlpine docker -OutputFile $env:USERPROFILE\\Downloads\\docker.tar.gz Distribution docker saved to C:\\Users\\AntoineMartin\\Downloads\\docker.tar.gz The size of the distribution file system is about 115 Mbytes: ","date":"11-05","objectID":"/posts/2022/05/11/multiple-docker-environments-on-windows/:5:0","tags":["windows","docker","wsl","powershell"],"title":"Multiple Docker environments on Windows","uri":"/posts/2022/05/11/multiple-docker-environments-on-windows/"},{"categories":["Development"],"content":"Creating another docker environment From the saved template, we can create a new docker environment that we name dockersandbox: PS\u003e Install-WslAlpine dockersandbox -SkipConfigure -RootFSURL file://$env:USERPROFILE/Downloads/docker.tar.gz ####\u003e Creating directory [C:\\Users\\AntoineMartin\\AppData\\Local\\dockersandbox]... ####\u003e Downloading file://C:\\Users\\AntoineMartin/Downloads/docker.tar.gz √¢‚Ä†‚Äô C:\\Users\\AntoineMartin\\AppData\\Local\\dockersandbox\\rootfs.tar.gz... ####\u003e Creating distribution [dockersandbox]... ####\u003e Done. Command to enter distribution: wsl -d dockersandbox PS\u003e Warning Important Confirguring the automatic start of openrc through wsl.conf doesn‚Äôt work (Windows 11 only). In consequence we need to start docker before using it: PS\u003e wsl -d dockersandbox -u root openrc default * Caching service dependencies ... [ ok ] * Starting Docker Daemon ... [ ok ] PS\u003e We can test docker in our new environment: PS\u003e wsl -d dockersandbox docker run --rm -it alpine:latest /bin/sh Unable to find image 'alpine:latest' locally latest: Pulling from library/alpine df9b9388f04a: Pull complete Digest: sha256:4edbd2beb5f78b1014028f4fbb99f3237d9561100b6881aabbf5acce2c4f9454 Status: Downloaded newer image for alpine:latest / # exit PS\u003e ","date":"11-05","objectID":"/posts/2022/05/11/multiple-docker-environments-on-windows/:6:0","tags":["windows","docker","wsl","powershell"],"title":"Multiple Docker environments on Windows","uri":"/posts/2022/05/11/multiple-docker-environments-on-windows/"},{"categories":["Development"],"content":"Switching between environments Add the following alias to %USERPROFILE%\\Documents\\WindowsPowerShell\\profile.ps1: function RunDockerInWsl { # Take $Env:DOCKER_WSL or 'docker' if undefined $DockerWSL = if ($null -eq $Env:DOCKER_WSL) { \"docker\" } else { $Env:DOCKER_WSL } # Try to find an existing distribution with the name $existing = Get-ChildItem HKCU:\\SOFTWARE\\Microsoft\\Windows\\CurrentVersion\\Lxss | Where-Object { $_.GetValue('DistributionName') -eq $DockerWSL } if ($null -eq $existing) { # Fail if the distribution doesn't exist throw \"The WSL distribution [$DockerWSL] does not exist !\" } else { # Ensure docker is started wsl -d $DockerWSL -u root openrc default # Perform the requested command wsl -d $DockerWSL /usr/bin/docker $args } } Set-Alias -Name docker -Value RunDockerInWsl It creates the docker alias. It will run docker command in the WSL distribution specified by the DOCKER_WSL environment variable, falling down to the docker WSL distribution if the variable is not defined. In consequence, switching from one environment to the other is easy : PS\u003e docker image ls * Caching service dependencies ... [ ok ] * Starting Docker Daemon ... [ ok ] REPOSITORY TAG IMAGE ID CREATED SIZE PS\u003e$Env:DOCKER_WSL=\"dockersandbox\" PS\u003edocker image ls REPOSITORY TAG IMAGE ID CREATED SIZE alpine latest 0ac33e5f5afa 5 weeks ago 5.57MB At this point, the two WSL distributions are running: PS\u003e wsl -l -v NAME STATE VERSION ... dockersandbox Running 2 docker Running 2 ... They can be stopped at will: PS\u003e wsl --terminate docker ","date":"11-05","objectID":"/posts/2022/05/11/multiple-docker-environments-on-windows/:7:0","tags":["windows","docker","wsl","powershell"],"title":"Multiple Docker environments on Windows","uri":"/posts/2022/05/11/multiple-docker-environments-on-windows/"},{"categories":["Development"],"content":"Wrapping up With WSL, having the ability to run Linux commands IN the current windows directory and treating distributions like docker containers provides a lot of flexibility and options when it comes to development environment setup and use. ","date":"11-05","objectID":"/posts/2022/05/11/multiple-docker-environments-on-windows/:8:0","tags":["windows","docker","wsl","powershell"],"title":"Multiple Docker environments on Windows","uri":"/posts/2022/05/11/multiple-docker-environments-on-windows/"},{"categories":["Python","Reference"],"content":"Python is an awesome language with an awesome ecosystem. It is both mature and very active. You are rarely left alone when you need to be doing something new. There are always one or more open source libraries or framework to help you achieve your goal. But now you‚Äôre left with a dilemma: Which one to choose ? The criteria are fairly simple. You want a library actively used and maintained, with the biggest community possible. But you want also a library still having momentum. You don‚Äôt want to invest too much is a technology on the downward slope. Without offense, people still doing Struts or ActionScript know what I mean. I have often been surprised by people selecting technologies only because it was the first they came around. In python, possibly because of PEP20, there are less options than with other languages. But for instance, in the frontend world, I have often been surprised of people not visiting npmtrends.com before choosing a SPA page router, given the number of alternatives available. The most used technology or the most trendy one may not suit your need and you may have a good reasong for picking another one, but you should do that knowingly. The following sites are the ones I found useful while trying to compare python libraries and frameworks technologies together. I put them here in order to remember them. You may find them useful for you. Don‚Äôt hesitate to tell me about the ones I forgot in the comments. ","date":"02-05","objectID":"/posts/2022/05/02/useful-python-packages-discovery-sites/:0:0","tags":["python","pypi","trends","libraries"],"title":"Useful Python packages discovery sites","uri":"/posts/2022/05/02/useful-python-packages-discovery-sites/"},{"categories":["Python","Reference"],"content":"Measuring community https://star-history.com/ Allows comparing the github stars progression of several projects. blacksheep never took off while fastapi gets closer to flask Stackoverflow trends data (https://insights.stackoverflow.com/trends) when available, shows medium to long term usage among newcomers. Only popular stuff will show up, however. flask has peeked during pandemic ","date":"02-05","objectID":"/posts/2022/05/02/useful-python-packages-discovery-sites/:1:0","tags":["python","pypi","trends","libraries"],"title":"Useful Python packages discovery sites","uri":"/posts/2022/05/02/useful-python-packages-discovery-sites/"},{"categories":["Python","Reference"],"content":"Looking at dependent projects I use https://www.wheelodex.org/ to look at the projects that depend on a candidate, to assess its users community size and maturity. For instance, at the time of this writing, 888 projects depend on FastAPI while 4,121 depend on Flask and 12,357 on click. For less outreaching projects, digging into the dependents may show that a lot of them have not left the experiment (version 0.0.1) stage. https://pydigger.com/ is about searching names in package metadata. As it sort answers by descending release dates, it allows assessing recent activity around a candidate. ","date":"02-05","objectID":"/posts/2022/05/02/useful-python-packages-discovery-sites/:2:0","tags":["python","pypi","trends","libraries"],"title":"Useful Python packages discovery sites","uri":"/posts/2022/05/02/useful-python-packages-discovery-sites/"},{"categories":["Python","Reference"],"content":"Measuring downloads https://pypistats.org gives official packages PyPi donwnload stats, by Python version, major and minor. It gives only stats for the last 6 months and the lack of moving averages makes the graphs hard to interpret. It‚Äôs only useful for young projects. pypistats example https://piptrends.com/ present the above stats but ressembles npmtrends, allowing comparing packages together. piptrends example https://packagegalaxy.com/ has smoother graphs that also make release dates appear. A download peak just after a release may show a feature breakthrough (see below). packagegalaxy example ","date":"02-05","objectID":"/posts/2022/05/02/useful-python-packages-discovery-sites/:3:0","tags":["python","pypi","trends","libraries"],"title":"Useful Python packages discovery sites","uri":"/posts/2022/05/02/useful-python-packages-discovery-sites/"},{"categories":["Python","Reference"],"content":"Others https://www.techempower.com/ benchmarks web frameworks, not only python. As with any benchmark, results need to be taken into account with a grain of salt. I used to visit https://pythonwheels.com to avoid using python packages that were not available as wheels. But a visit to the site now makes clear that there are not much left. ","date":"02-05","objectID":"/posts/2022/05/02/useful-python-packages-discovery-sites/:4:0","tags":["python","pypi","trends","libraries"],"title":"Useful Python packages discovery sites","uri":"/posts/2022/05/02/useful-python-packages-discovery-sites/"},{"categories":["Python","Reference"],"content":"Conclusion The above sites and information are just to know where you stand before investing your time and energy on a new toy. Sometimes however, you will go against the hints that the above sites give you (I‚Äôm looking at you, poetry). ","date":"02-05","objectID":"/posts/2022/05/02/useful-python-packages-discovery-sites/:5:0","tags":["python","pypi","trends","libraries"],"title":"Useful Python packages discovery sites","uri":"/posts/2022/05/02/useful-python-packages-discovery-sites/"},{"categories":["Development","Android"],"content":"With In-App Billing on Android, each time a purchase occurs, your application receives a JSON payload containing information about the purchase, as well as its signature with your developer certificate. Google encourages you to verify that the signature is valid to authentify the purchase. You can do that inside the application, but if the delivery of the purchase involves a server, it is better to do it on the server to prevent client code manipulation. The following show how to do it on .Net server application. The JSON payload looks like the following : { \"nonce\" : 1836535032137741465, \"orders\" : [{ \"notificationId\" : \"android.test.purchased\", \"orderId\" : \"transactionId.android.test.purchased\", \"packageName\" : \"com.example.dungeons\", \"productId\" : \"android.test.purchased\", \"developerPayload\" : \"bGoa+V7g/yqDXvKRqq+JTFn4uQZbPiQJo4pf9RzJ\", \"purchaseTime\" : 1290114783411, \"purchaseState\" : 0, \"purchaseToken\" : \"rojeslcdyyiapnqcynkjyyjh\" }] } You receive it with a broadcast com.android.vending.billing.PURCHASE_STATE_CHANGED in the inapp_signed_data extra intent field. The signature comes as a base 64 encoded string in the inapp_signature intent field and looks like this : YlNBaqlKSS+zk/fteJuHbvI3/N+hbiLiolYsMl8gCD13+Ii+1m4GSd68rc2TwbSLYsYrHVL/9xg/0CBf CN6NKLtqjFqRs034ExCW2qaMddwfRiqsGZ3z7ZvWuMyNntE3pTGTxG2X/71/cpGwQoSFQBceVR9t5Sge Tw5HJimt5xlIhHqgRxS/W/kfrJIyKt03l2hUJDGOX9eig5S4ex6fgyFZxR73/HxOFGJ9ohApwaBNF7rD LaMZFnYbLsYgBWMOHW1uE+F5b2JZWvyColpe5SKMWaNVWVWZGte1WBOYRFxbriZR1VwClkEg9Y4mVn5k SZIje5pSueLKwiForU02jA== It is the signature of the SHA1 digest of the JSON payload with the private key of your developer certificate. Don‚Äôt look for this private key, it is detained by Google. Google only provides you the corresponding public key in the profile page of your developer account : This public key is the base 64 string of the Subject Public Key Info of your certificate encoded in the DER format. It corresponds to the following part of your certificate: Certificate: ... Subject Public Key Info: Public Key Algorithm: rsaEncryption RSA Public Key: (1024 bit) Modulus (1024 bit): 00:b4:31:98:0a:c4:bc:62:c1:88:aa:dc:b0:c8:bb: 33:35:19:d5:0c:64:b9:3d:41:b2:96:fc:f3:31:e1: 66:36:d0:8e:56:12:44:ba:75:eb:e8:1c:9c:5b:66: 70:33:52:14:c9:ec:4f:91:51:70:39:de:53:85:17: 16:94:6e:ee:f4:d5:6f:d5:ca:b3:47:5e:1b:0c:7b: c5:cc:2b:6b:c1:90:c3:16:31:0d:bf:7a:c7:47:77: 8f:a0:21:c7:4c:d0:16:65:00:c1:0f:d7:b8:80:e3: d2:75:6b:c1:ea:9e:5c:5c:ea:7d:c1:a1:10:bc:b8: e8:35:1c:9e:27:52:7e:41:8f Exponent: 65537 (0x10001) ... The public key in this format cannot be read directly by the RSACryptoServiceProvider class of the .Net System.Security.Cryptography module. The preferred import format for this class is XML. The Bouncy Castle Library allows reading this kind of encoding, but you don‚Äôt really need to add a new dependency to your project. Instead, what you need is simply to convert your public key in XML. Once this is done, you can use the following simple .Net code to check the signature: public static bool verify(String message, String base64Signature, String publicKey) { // By default the result is false bool result = false; try { // Create the provider and load the KEY RSACryptoServiceProvider provider = new RSACryptoServiceProvider(); provider.FromXmlString(publicKey); // The signature is supposed to be encoded in base64 and the SHA1 checksum // Of the message is computed against the UTF-8 representation of the // message byte[] signature = Convert.FromBase64String(base64Signature); SHA1Managed sha = new SHA1Managed(); byte[] data = Encoding.UTF8.GetBytes(message); result = provider.VerifyData(data, sha, signature); } catch (Exception /* e */) { /* TODO: add some kind of logging here */} return result; } For converting your key, you can download the PEMKeyLoader class and use it in a Console Project to convert your key to XML with the following code : ... RSACryptoServiceProvider provider = PEMKeyLoader.CryptoServiceProviderFromPublicKeyInfo(MY_BASE64_PUB","date":"15-11","objectID":"/posts/2012/11/15/checking-google-play-signatures-on-net/:0:0","tags":["old","android","google play",".net"],"title":"Checking Google Play Signatures on .Net","uri":"/posts/2012/11/15/checking-google-play-signatures-on-net/"},{"categories":["DevOps"],"content":"Redmine can show the timeline of a Git repository but this repository needs to be local (see here). When you host your repository externally (on GitHub, for instance), you need to synchronize your remote repository on your Redmine server. The following shell script is an All in one command that can be easily put in the crontab to mirror the repository on your Redmine server : #!/bin/sh if [ \"run\" != \"$1\" ]; then exec ssh -i \"$GIT_KEY\" -o \"StrictHostKeyChecking no\" \"$@\" fi remote=$2 local=$3 echo \"Mirroring from $remote to $local\" name=$(basename \"$local\") export GIT_KEY=\"`mktemp /tmp/git.XXXXXX`\" export GIT_SSH=\"$0\" cat \u003e\"$GIT_KEY\" \u003c\u003cEOF -----BEGIN DSA PRIVATE KEY----- ### Put here your private key ### -----END DSA PRIVATE KEY----- EOF if [ -d \"$local\" ]; then git \"--git-dir=$local\" remote update else git clone --mirror \"$remote\" \"$local\" fi rm -f \"$GIT_KEY\" exit 0 You need to copy the private key in the script (line 20). You can then use the script with the following syntax git-import.sh run \u003cremote_repository\u003e \u003clocal_repository\u003e Notice the use of the run argument to distinguish between executions of the script as a user and as the ssh command to be used by Git. Here is an example: [antoine@dev ~]$ ./git-import.sh run git@github.com:antoinemartin/django-windows-tools.git django-windows-tools.git Mirroring from git@github.com:antoinemartin/django-windows-tools.git to django-windows-tools.git Cloning into bare repository 'django-windows-tools.git'... remote: Counting objects: 112, done. remote: Compressing objects: 100% (88/88), done. remote: Total 112 (delta 46), reused 78 (delta 14) Receiving objects: 100% (112/112), 41.04 KiB, done. Resolving deltas: 100% (46/46), done. [antoine@dev ~]$ The first time you run the script, it creates the Git mirror. The following runs only syncs the mirror: [antoine@dev ~]$ ./git-import.sh run git@github.com:antoinemartin/django-windows-tools.git django-windows-tools.git Mirroring from git@github.com:antoinemartin/django-windows-tools.git to django-windows-tools.git Fetching origin [antoine@dev ~]$ ","date":"15-11","objectID":"/posts/2012/11/15/mirror-a-git-repository-through-ssh/:0:0","tags":["git","ssh","redmine","bash"],"title":"Mirror a Git Repository Through Ssh","uri":"/posts/2012/11/15/mirror-a-git-repository-through-ssh/"},{"categories":["Development","Android","Test"],"content":"The Android Test Framework provides many tools to test parts of an Android application, and the ServiceTestCase in particular to test your Service classes. This class is quite useful but you may find yourself scratching your head because your test does not work like it should. This happens in particular if you‚Äôre doing some background work in your service, relying for example on AsyncTask for it. Read on if you want to understand why it doesn‚Äôt work and find a solution for it. In an Android application, any service is instantiated and operates on the main thread. But this is not the case in the test framework provided by the ServiceTestCase class. Your Service is instantiated in the same thread the test runs. While your tests are running, there is no Looper waiting for messages on the service thread. In consequence, anything that relies on it and on the Handler class to communicate back to the main thread will not work. For instance, AsyncTask uses a handler to ensure that the onPostExecute method is called on the main thread. After doInBackground has been called, it posts a message on this handler, but as the Looper on the service is not running to handle the message, the onPostExecute method will never be called. To circumvent this behaviour, the service must be run on a separate thread with a Looper running. ","date":"08-11","objectID":"/posts/2012/11/08/avoid-thread-issues-while-testing-an-android-service/:0:0","tags":["old","obsolete","android","test","development"],"title":"Avoid Thread Issues While Testing an Android Service","uri":"/posts/2012/11/08/avoid-thread-issues-while-testing-an-android-service/"},{"categories":["Development","Android","Test"],"content":"Simulating main thread behaviour The ThreadServiceTestCase\u003cT extends Service\u003e (source here) class that we describe here provides such features. It declares a Looper and a Hanlder to be able to run code on it: protected Handler serviceHandler; protected Looper serviceLooper; In the setup of the test, we instantiate the service thread, start it, and link our handler with its looper: @Override protected void setUp() throws Exception { super.setUp(); // Setup service thread HandlerThread serviceThread = new HandlerThread(\"[\" + serviceClass.getSimpleName() + \"Thread]\"); serviceThread.start(); serviceLooper = serviceThread.getLooper(); serviceHandler = new Handler(serviceLooper); } The corresponding tearDown method shuts down the tread. We provide a runOnServiceThread method to be able to run code on the service thread: protected void runOnServiceThread(final Runnable r) { final CountDownLatch serviceSignal = new CountDownLatch(1); serviceHandler.post(new Runnable() { @Override public void run() { r.run(); serviceSignal.countDown(); } }); try { serviceSignal.await(); } catch (InterruptedException ie) { fail(\"The Service thread has been interrupted\"); } } Then, the startService methods starts the service in its own thread: static class Holder\u003cH\u003e { H value; } protected T startService(final boolean bound, final ServiceRunnable r) { final Holder\u003cT\u003e serviceHolder = new Holder\u003cT\u003e(); // I want to create my service in its own 'Main thread' // So it can use its handler runOnServiceThread(new Runnable() { @Override public void run() { T service = null; if (bound) { /* IBinder binder = */bindService(new Intent(getContext(), serviceClass)); } else { startService(new Intent(getContext(), serviceClass)); } service = getService(); if (r != null) r.run(service); serviceHolder.value = service; } }); return serviceHolder.value; } The bound parameters tells whether to start the service with a binding or with an Intent.. The optional ServiceRunnable parameter can be provided to add some initialization code. A test class using this code looks like the following: public class MyServiceTest extends ThreadServiceTestCase\u003cMyService\u003e { public MyServiceTest() { super(MyService.class); } public void testSomething() { // starts the service MyService service = startService(true, null); ... // Do something on the service runOnServiceThread( new ServiceRunnable() { public void run(Service service) { // do something } }); } With it, the service is started in its own thread and the Looper and Handler mechanism will work. ","date":"08-11","objectID":"/posts/2012/11/08/avoid-thread-issues-while-testing-an-android-service/:1:0","tags":["old","obsolete","android","test","development"],"title":"Avoid Thread Issues While Testing an Android Service","uri":"/posts/2012/11/08/avoid-thread-issues-while-testing-an-android-service/"},{"categories":["Development","Android","Test"],"content":"Waiting for listeners to be notified A service that performs tasks asynchronously also notifies the outcome of the background tasks asynchronously. There are several techniques for doing that, but the most common are : Broadcast an intent, or Call a callback method on listeners. This usually happens in the main thread. In our case, it would happen in the service thread. As the test is executing itself in its own thread, some synchronization mechanism is needed between the service thread and the test thread to be able to handle the outcome of the background task in the test. The ThreadServiceTestCase class provides an helper class for that: public static class ServiceSyncHelper { // The semaphore will wakeup clients protected final Semaphore semaphore = new Semaphore(0); /** * Waits for some response coming from the service. * * @param timeout * The maximum time to wait. * @throws InterruptedException * if the Thread is interrupted or reaches the timeout. */ public synchronized void waitListener(long timeout) throws InterruptedException { if (!semaphore.tryAcquire(timeout, TimeUnit.MILLISECONDS)) throw new InterruptedException(); } } It contains a semaphore that can be used to synchronize the service thread with the test thread. In the case of the callback listener, we can then define an utility class like the following: static class SynchronizedListener extends ServiceSyncHelper { Object result; /** * Service listener that registers the value returned by the service in * the holder and release the semaphore. */ final MyService.Listener listener = new MyService.Listener() { public void onTaskPerformed(MyService service, Object returnValue) { result = returnValue; semaphore.release(); } }; } When notified by the service in the service thread, the contained listener releases the semaphore and awakes the test that is waiting on the semaphore. The listener contained in the helper class also needs to be added to the service being tested at service initialization. A test using this feature then becomes : public void testSomething() { // Catch listener callback in the test final SynchronizedListener listener = new SynchronizedListener(); // starts the service /* MyService service = */ startService(true, new ServiceRunnable() { @Override public void run(Service service) { // add our listener to the service service.addListener(listener); } }); // wait for the service to notify us try { // Wait for the service to perform its background task listener.waitListener(WAIT_TIME); } catch (InterruptedException ie) { fail(\"The listener never got signaled\"); } assertEquals(expected_value, listener.result ); ... } You can grab The ThreadServiceTestCase\u003cT extends Service\u003e source code here. Hope it will help. ","date":"08-11","objectID":"/posts/2012/11/08/avoid-thread-issues-while-testing-an-android-service/:2:0","tags":["old","obsolete","android","test","development"],"title":"Avoid Thread Issues While Testing an Android Service","uri":"/posts/2012/11/08/avoid-thread-issues-while-testing-an-android-service/"},{"categories":["Development","Android"],"content":"Having an unlocked and rooted device provides several advantages : Easy backup and restore with Nandroid backup, Easy firmware replacement and updates installation, Advanced debugging capabilities. The following instructions allow unlocking and rooting a Nexus device (Galaxy Nexus, Nexus 7) from the command line on a Linux machine. It involves: Backuping your device, Unlocking the bootloader, Restoring the backup, Rooting the device. ","date":"25-10","objectID":"/posts/2012/10/25/unlock-and-root-a-nexus-device/:0:0","tags":["old","obsolete","android","linux","adb","root"],"title":"Unlock and Root a Nexus Device","uri":"/posts/2012/10/25/unlock-and-root-a-nexus-device/"},{"categories":["Development","Android"],"content":"Prerequisites Here is the list of prerequisites : Android SDK, to have access to adb and fastboot. Clockwork Mode (CWM) recovery image. SuperSU installable zip. The platform-tools directory of the Android SDK must be on your PATH, and the device must have USB debugging enabled. ","date":"25-10","objectID":"/posts/2012/10/25/unlock-and-root-a-nexus-device/:0:1","tags":["old","obsolete","android","linux","adb","root"],"title":"Unlock and Root a Nexus Device","uri":"/posts/2012/10/25/unlock-and-root-a-nexus-device/"},{"categories":["Development","Android"],"content":"Udev rules On Linux, you don‚Äôt have to install any driver. You need however to enable access for your users. Depending on your distribution, you may have a package handling that, but if not, here is a quick way to give access to your user to the device (here antoine). Type as root: [root@dev ~] $ cd /etc/udev/rules.d [root@dev rules.d] $ wget https://raw.github.com/M0Rf30/android-udev-rules/master/51-android.rules [root@dev rules.d] $ groupadd adbusers [root@dev rules.d] $ udevadm control --reload-rules [root@dev rules.d] $ gpasswd -a antoine adbusers Plug your device on the USB cable, and you should be able to see it with adb: [antoine@dev nexus] $ adb devices List of devices attached 015d2ebecd341e06 device ","date":"25-10","objectID":"/posts/2012/10/25/unlock-and-root-a-nexus-device/:0:2","tags":["old","obsolete","android","linux","adb","root"],"title":"Unlock and Root a Nexus Device","uri":"/posts/2012/10/25/unlock-and-root-a-nexus-device/"},{"categories":["Development","Android"],"content":"Backup Unlocking the bootloader wipes all the data on the device. If you‚Äôre not doing this on a new device, you may want to backup and restore your data and applications. With the device connected in USB debug mode, type : [antoine@dev nexus] $ adb backup -apk -shared -all -f backup.ab Depending on the amount of data you have on your device, this process can be quite long. ","date":"25-10","objectID":"/posts/2012/10/25/unlock-and-root-a-nexus-device/:0:3","tags":["old","obsolete","android","linux","adb","root"],"title":"Unlock and Root a Nexus Device","uri":"/posts/2012/10/25/unlock-and-root-a-nexus-device/"},{"categories":["Development","Android"],"content":"OEM unlock Unlocking the device is easy. With the device connected in USB debug mode, type: [antoine@dev nexus] $ adb reboot bootloader The device will reboot in fastboot mode. To check this, type: [antoine@dev nexus] $ fastboot devices 015d2ebecd341e06 fastboot You will see your device in the list. Then you can unlock it by typing: [antoine@dev nexus] $ fastboot oem unlock ... (bootloader) erasing userdata... (bootloader) erasing userdata done (bootloader) erasing cache... (bootloader) erasing cache done (bootloader) unlocking... (bootloader) Bootloader is unlocked now. OKAY [ 54.821s] finished. total time: 54.821s [antoine@dev nexus] $ fastboot reboot At the end of the process, reboot your device: [antoine@dev nexus] $ fastboot reboot You will go through the initialization process in the device. ","date":"25-10","objectID":"/posts/2012/10/25/unlock-and-root-a-nexus-device/:0:4","tags":["old","obsolete","android","linux","adb","root"],"title":"Unlock and Root a Nexus Device","uri":"/posts/2012/10/25/unlock-and-root-a-nexus-device/"},{"categories":["Development","Android"],"content":"Restore Once the device is up and running, you can restore your data with: [antoine@dev nexus] $ adb restore backup.ab ","date":"25-10","objectID":"/posts/2012/10/25/unlock-and-root-a-nexus-device/:0:5","tags":["old","obsolete","android","linux","adb","root"],"title":"Unlock and Root a Nexus Device","uri":"/posts/2012/10/25/unlock-and-root-a-nexus-device/"},{"categories":["Development","Android"],"content":"Root To root the device, we will apply the SuperSU installable zip as an update in CWM. First we push the SuperSU installable zip in the device filesystem: [antoine@dev nexus] $ adb push CWM-SuperSU-v0.96.zip /sdcard/update.zip 752 KB/s (674673 bytes in 0.875s) Now that the device is unlocked, we can boot it into CWM. We first reboot it in fastboot mode: [antoine@dev nexus] $ adb reboot bootloader And then boot it with CWM: [antoine@dev nexus] $ fastboot boot recovery-clockwork-6.0.1.0-grouper.img downloading 'boot.img'... OKAY [ 0.800s] booting... OKAY [ 0.020s] finished. total time: 0.820s After a few seconds, the device will show the CWM interface. With the volume buttons, move to the install zip from sdcard option and select it by pushing the power button. On the new menu that appears, choose the apply /sdcard/update.zip option and scroll down to the Yes option. click on the power button and SuperSU will be installed. Once done, you can go back to the main CWM menu and reboot the device. ","date":"25-10","objectID":"/posts/2012/10/25/unlock-and-root-a-nexus-device/:0:6","tags":["old","obsolete","android","linux","adb","root"],"title":"Unlock and Root a Nexus Device","uri":"/posts/2012/10/25/unlock-and-root-a-nexus-device/"},{"categories":["Development","Android"],"content":"Permanently install CWM You can permanently install CWM on the device recovery partition so that you can start your device in CWM without being connected via USB. Your device automatically restores the recovery partition at each boot. To avoid that, you need to delete the /system/recovery-from-boot.p file on the device : [antoine@dev nexus] $ adb shell shell@android:/ $ su shell@android:/ # rm /system/recovery-from-boot.p shell@android:/ # exit shell@android:/ $ exit You can then reboot in fastboot mode and install CWM permanently : [antoine@dev nexus] $ adb reboot bootloader [antoine@dev nexus] $ fastboot flash recovery recovery-clockwork-touch-6.0.0.6-grouper.img [antoine@dev nexus] $ fastboot reboot I personally don‚Äôt recommend to install CWM permanently as it will prevent you from installing the OTA updates that are pushed to your device. ","date":"25-10","objectID":"/posts/2012/10/25/unlock-and-root-a-nexus-device/:0:7","tags":["old","obsolete","android","linux","adb","root"],"title":"Unlock and Root a Nexus Device","uri":"/posts/2012/10/25/unlock-and-root-a-nexus-device/"},{"categories":["DevOps"],"content":"So you have this brand new project my_project of yours with your local Git repository set up and you want to quickly make it available for others to clone on your repository server. All your projects are located in your server git.mycompany.com under /srv/git. You‚Äôre using the user named git to connect to your server with the SSH private key located in ~/.ssh/git. Here is the quickest way to deploy your project: You first add your SSH key to the SSH agent : [antoine@dev my_project] $ ssh-add ~/.ssh/git If the agent is not started, you need to execute first : [antoine@dev my_project] $ eval `ssh-agent` Then you create an empty Git bare repository on your server with the name of your project : [antoine@dev my_project] $ ssh git@git.mycompany.com \"git --bare init /srv/git/$(basename $(pwd)).git\" Initialized empty Git repository in /srv/git/my_project.git/ Then you add your newly created remote Git repository as the origin of your local repo : [antoine@dev my_project] $ git remote add origin \"git@git.mycompany.com:/srv/git/$(basename $(pwd)).git\" You push your master branch to the remote repository : [antoine@dev my_project] $ git push origin master Counting objects: 3, done. Writing objects: 100% (3/3), 209 bytes, done. Total 3 (delta 0), reused 0 (delta 0) To git@git.mycompany.com:/srv/git/my_project.git * [new branch] master -\u003e master Lastly, you make your local branch track your remote branch : [antoine@dev my_project] $ git branch --set-upstream master origin/master Branch master set up to track remote branch master from origin. The last two steps can be done for any local branch you have that you want to push on the server. You can test pulling from the server : [antoine@dev my_project] $ git pull Already up-to-date. That‚Äôs it ! ","date":"24-10","objectID":"/posts/2012/10/24/quickly-deploy-a-git-project-on-a-server-with-ssh/:0:0","tags":["old","git","bash","ssh","linux","macos"],"title":"Quickly Deploy a Git Project on a Server With Ssh","uri":"/posts/2012/10/24/quickly-deploy-a-git-project-on-a-server-with-ssh/"},{"categories":["DevOps"],"content":"I needed recently to install the excellent project management tool Redmine on a CentOS 6.2 machine. There are some tutorials on the Web (here or here) but they are a little bit outdated. The following is a method that works as of today. ","date":"06-07","objectID":"/posts/2012/07/06/installing-redmine-on-centos-6-dot-2-with-mysql-and-apache/:0:0","tags":["old","obsolete","linux","redmine","centos","ruby"],"title":"Installing Redmine on Centos 6 Dot 2 With Mysql and Apache","uri":"/posts/2012/07/06/installing-redmine-on-centos-6-dot-2-with-mysql-and-apache/"},{"categories":["DevOps"],"content":"Pre-requisites Logged as root, install the following packages: yum install make gcc gcc-c++ zlib-devel ruby-devel rubygems ruby-libs apr-devel apr-util-devel httpd-devel mysql-devel mysql-server automake autoconf ImageMagick ImageMagick-devel curl-devel And then install the bundle ruby gem: gem install bundle ","date":"06-07","objectID":"/posts/2012/07/06/installing-redmine-on-centos-6-dot-2-with-mysql-and-apache/:1:0","tags":["old","obsolete","linux","redmine","centos","ruby"],"title":"Installing Redmine on Centos 6 Dot 2 With Mysql and Apache","uri":"/posts/2012/07/06/installing-redmine-on-centos-6-dot-2-with-mysql-and-apache/"},{"categories":["DevOps"],"content":"Install Redmine Run the following commands to install Redmine: cd /var/www wget http://rubyforge.org/frs/download.php/76255/redmine-1.4.4.tar.gz tar zxf redmine-1.4.4.tar.gz ln -s redmine-1.4.4 redmine rm -f redmine-1.4.4.tar.gz ","date":"06-07","objectID":"/posts/2012/07/06/installing-redmine-on-centos-6-dot-2-with-mysql-and-apache/:2:0","tags":["old","obsolete","linux","redmine","centos","ruby"],"title":"Installing Redmine on Centos 6 Dot 2 With Mysql and Apache","uri":"/posts/2012/07/06/installing-redmine-on-centos-6-dot-2-with-mysql-and-apache/"},{"categories":["DevOps"],"content":"Install Redmine ruby dependencies Bundle helps us install the ruby Redmine dependencies: cd /var/www/redmine bundle install --without postgresql sqlite test development ","date":"06-07","objectID":"/posts/2012/07/06/installing-redmine-on-centos-6-dot-2-with-mysql-and-apache/:3:0","tags":["old","obsolete","linux","redmine","centos","ruby"],"title":"Installing Redmine on Centos 6 Dot 2 With Mysql and Apache","uri":"/posts/2012/07/06/installing-redmine-on-centos-6-dot-2-with-mysql-and-apache/"},{"categories":["DevOps"],"content":"Database creation First we start MySQL: service mysqld start Then we secure it (Optional): mysql_secure_installation We then create the redmine database and user: $ mysql mysql\u003e create database redmine character set utf8; mysql\u003e grant all privileges on redmine.* to 'redmine'@'localhost' identified by 'my_password'; mysql\u003e flush privileges; mysql\u003e quit ","date":"06-07","objectID":"/posts/2012/07/06/installing-redmine-on-centos-6-dot-2-with-mysql-and-apache/:4:0","tags":["old","obsolete","linux","redmine","centos","ruby"],"title":"Installing Redmine on Centos 6 Dot 2 With Mysql and Apache","uri":"/posts/2012/07/06/installing-redmine-on-centos-6-dot-2-with-mysql-and-apache/"},{"categories":["DevOps"],"content":"Redmine database configuration We copy the database configuration example and we modify it to point to our newly created database: cd /var/www/redmine/config copy database.yml.example database.yml On the database.yml file, the production section should look like this: production: adapter: mysql database: redmine host: localhost username: redmine password: my_password encoding: utf8 And then we create and populate the database with the following rake commands: cd /var/www/redmine rake generate_session_store rake db:migrate RAILS_ENV=\"production\" rake redmine:load_default_data RAILS_ENV=\"production\" ","date":"06-07","objectID":"/posts/2012/07/06/installing-redmine-on-centos-6-dot-2-with-mysql-and-apache/:5:0","tags":["old","obsolete","linux","redmine","centos","ruby"],"title":"Installing Redmine on Centos 6 Dot 2 With Mysql and Apache","uri":"/posts/2012/07/06/installing-redmine-on-centos-6-dot-2-with-mysql-and-apache/"},{"categories":["DevOps"],"content":"Outgoing email configuration (Optional) To configure an outgoing SMTP server for sending emails, we create the config/configuration.yml file from the sample: cd /var/www/redmine/config cp configuration.yml.example configuration.yml And edit it to provide our configuration : production: email_delivery: delivery_method: :smtp smtp_settings: address: 'smtp.mydomain.com' port: 25 domain: 'mydomain.com' ","date":"06-07","objectID":"/posts/2012/07/06/installing-redmine-on-centos-6-dot-2-with-mysql-and-apache/:6:0","tags":["old","obsolete","linux","redmine","centos","ruby"],"title":"Installing Redmine on Centos 6 Dot 2 With Mysql and Apache","uri":"/posts/2012/07/06/installing-redmine-on-centos-6-dot-2-with-mysql-and-apache/"},{"categories":["DevOps"],"content":"Redmine standalone testing At this point, Redmine can be tested in standalone mode by running the following command: cd /var/www/redmine/ ruby script/server webrick -e production and open the http://localhost:3000 address in a browser. If you are testing from another computer, you will need to open the port in the /etc/sysconfig/iptables file by duplicating the ssh (port 22) line and adapting it: -A INPUT -m state --state NEW -m tcp -p tcp --dport 22 -j ACCEPT -A INPUT -m state --state NEW -m tcp -p tcp --dport 3000 -j ACCEPT Then apply the new configuration with the following command: service iptables restart ","date":"06-07","objectID":"/posts/2012/07/06/installing-redmine-on-centos-6-dot-2-with-mysql-and-apache/:7:0","tags":["old","obsolete","linux","redmine","centos","ruby"],"title":"Installing Redmine on Centos 6 Dot 2 With Mysql and Apache","uri":"/posts/2012/07/06/installing-redmine-on-centos-6-dot-2-with-mysql-and-apache/"},{"categories":["DevOps"],"content":"Passenger installation To install Phusion passenger, we first install its gem: gem install passenger And then install the Apache module with the command: passenger-install-apache2-module ","date":"06-07","objectID":"/posts/2012/07/06/installing-redmine-on-centos-6-dot-2-with-mysql-and-apache/:8:0","tags":["old","obsolete","linux","redmine","centos","ruby"],"title":"Installing Redmine on Centos 6 Dot 2 With Mysql and Apache","uri":"/posts/2012/07/06/installing-redmine-on-centos-6-dot-2-with-mysql-and-apache/"},{"categories":["DevOps"],"content":"Apache configuration We remove the default Apache configuration and replace it by a new one: cd /etc/httpd mv conf.d available mkdir conf.d In the empty new conf.d folder, we create a redmine.conf file with the following configuration: # Loading Passenger LoadModule passenger_module /usr/lib/ruby/gems/1.8/gems/passenger-3.0.13/ext/apache2/mod_passenger.so PassengerRoot /usr/lib/ruby/gems/1.8/gems/passenger-3.0.13 PassengerRuby /usr/bin/ruby \u003cVirtualHost *:80\u003e ServerName redmine.mycompany.com DocumentRoot /var/www/redmine/public \u003cDirectory /var/www/redmine/public\u003e # This relaxes Apache security settings. AllowOverride all # MultiViews must be turned off. Options -MultiViews allow from all \u003c/Directory\u003e ErrorLog \"|/usr/sbin/rotatelogs /etc/httpd/logs/redmine-error.%Y-%m-%d.log 86400\" CustomLog \"|/usr/sbin/rotatelogs /etc/httpd/logs/redmine-access.%Y-%m-%d.log 86400\" \"%h %l %u %t %D \\\"%r\\\" %\u003es %b \\\"%{Referer}i\\\" \\\"%{User-Agent}i\\\"\" \u003c/VirtualHost\u003e We then enable named based virtual hosting for our server by uncommenting the following line in the /etc/httpd/conf/httpd.conf file: ... # # Use name-based virtual hosting. # NameVirtualHost *:80 ... We give full access on the redmine folder to the apache user and test the configuration: chown -R apache:root /var/www/redmine service httpd configtest At this point, the SELinux configuration needs to be modified to allow our apache instance to run the phusion passenger module. You can do this by putting SELinux in permissive mode: setenfore Permissive And letting the Permissive mode survive a reboot by modifying the /etc/selinux/config file from: SELINUX=enforcing to SELINUX=permissive If you want to run redmine while enforcing, you may want to apply the method described here for which you will need to install the policycoreutils-python package. In any case, you will start Apache with the command: service httpd start Now you can access your Redmine installation with your browser. To access it from all the computers in your network, you will need to open the port 80 in the /etc/sysconfig/iptables. You can replace the 3000 rule by : -A INPUT -m state --state NEW -m tcp -p tcp --dport 22 -j ACCEPT -A INPUT -m state --state NEW -m tcp -p tcp --dport 80 -j ACCEPT And restart iptables. service iptables restart ","date":"06-07","objectID":"/posts/2012/07/06/installing-redmine-on-centos-6-dot-2-with-mysql-and-apache/:9:0","tags":["old","obsolete","linux","redmine","centos","ruby"],"title":"Installing Redmine on Centos 6 Dot 2 With Mysql and Apache","uri":"/posts/2012/07/06/installing-redmine-on-centos-6-dot-2-with-mysql-and-apache/"},{"categories":["DevOps"],"content":"Start services at boot To have MySQL and Apache started at boot, run the commands: chkconfig --level 345 mysqld on chkconfig --level 345 httpd on ","date":"06-07","objectID":"/posts/2012/07/06/installing-redmine-on-centos-6-dot-2-with-mysql-and-apache/:10:0","tags":["old","obsolete","linux","redmine","centos","ruby"],"title":"Installing Redmine on Centos 6 Dot 2 With Mysql and Apache","uri":"/posts/2012/07/06/installing-redmine-on-centos-6-dot-2-with-mysql-and-apache/"},{"categories":["DevOps"],"content":"Cleaning up A quick command to clean up all the devel stuff needed for installation: yum remove '*-devel' make automake autoconf ","date":"06-07","objectID":"/posts/2012/07/06/installing-redmine-on-centos-6-dot-2-with-mysql-and-apache/:11:0","tags":["old","obsolete","linux","redmine","centos","ruby"],"title":"Installing Redmine on Centos 6 Dot 2 With Mysql and Apache","uri":"/posts/2012/07/06/installing-redmine-on-centos-6-dot-2-with-mysql-and-apache/"},{"categories":["DevOps"],"content":"Tips Don‚Äôt forget that if you change your Redmine configuration, you don‚Äôt have to restart Apache. Your can restart only Redmine with the command: touch /var/www/redmine/tmp/restart.txt If you restore data on your server from another redmine instance that runs on a previous version, dont forget to migrate your data: cd /var/www/redmine rake db:migrate RAILS_ENV=\"production\" ","date":"06-07","objectID":"/posts/2012/07/06/installing-redmine-on-centos-6-dot-2-with-mysql-and-apache/:12:0","tags":["old","obsolete","linux","redmine","centos","ruby"],"title":"Installing Redmine on Centos 6 Dot 2 With Mysql and Apache","uri":"/posts/2012/07/06/installing-redmine-on-centos-6-dot-2-with-mysql-and-apache/"},{"categories":["DevOps","Django"],"content":"In my previous post, I showed how to set up a Django project on a Windows Server to be served behind IIS. After setting up the server, the next thing we want with a Django application is to be able to run background and scheduled tasks, and Celery is the perfect tool for that. On Windows, background processes are mostly run as Windows Services. Fortunately, Python for Windows Extensions (a.k.a pywin32) provides facilities to create a Windows Service. I have packaged the related code for this post and the previous one in a project called django-windows-tools available on github and the cheese shop. To make it available for your application, simply install it with the command: pip install django-windows-tools ","date":"04-07","objectID":"/posts/2012/07/04/django-on-windows-run-celery-as-a-windows-service/:0:0","tags":["old","obsolete","django","celery","windows"],"title":"Django on Windows: Run Celery as a Windows Service","uri":"/posts/2012/07/04/django-on-windows-run-celery-as-a-windows-service/"},{"categories":["DevOps","Django"],"content":"Configuring your project To run Celery for your project, you need to install Celery and choose a Broker for passing messages between the Django application and the Celery worker processes. Installation of celery is easy: \u003e pip install django-celery Then you add it to your settings.py: INSTALLED_APPS += ( 'djcelery', ) import djcelery djcelery.setup_loader() You can choose among several message brokers. I personally use a Windows port of Redis installed as a Windows Service. The advantage of Redis is that it can also be used as an in-memory database. In case you‚Äôre interested, you can find here a binary copy of my installation. The configuration of Redis as Celery‚Äôs broker also occurs in the settings.py: # Redis configuration REDIS_PORT=6379 REDIS_HOST = \"127.0.0.1\" REDIS_DB = 0 REDIS_CONNECT_RETRY = True # Broker configuration BROKER_HOST = \"127.0.0.1\" BROKER_BACKEND=\"redis\" BROKER_USER = \"\" BROKER_PASSWORD =\"\" BROKER_VHOST = \"0\" # Celery Redis configuration CELERY_SEND_EVENTS=True CELERY_RESULT_BACKEND='redis' CELERY_REDIS_HOST='127.0.0.1' CELERY_REDIS_PORT=6379 CELERY_REDIS_DB = 0 CELERY_TASK_RESULT_EXPIRES = 10 CELERYBEAT_SCHEDULER=\"djcelery.schedulers.DatabaseScheduler\" CELERY_ALWAYS_EAGER=False Finally, you add the django_windows_tools application to your project: INSTALLED_APPS += ( 'django_windows_tools', ) After the configuration, a python manage.py syncdb will ensure that the database of your project is up to date. ","date":"04-07","objectID":"/posts/2012/07/04/django-on-windows-run-celery-as-a-windows-service/:1:0","tags":["old","obsolete","django","celery","windows"],"title":"Django on Windows: Run Celery as a Windows Service","uri":"/posts/2012/07/04/django-on-windows-run-celery-as-a-windows-service/"},{"categories":["DevOps","Django"],"content":"Enabling the service The installed service is going to allow us to run in the background arbitrary management commands related to our project. With the application installed, on the root of your project, type the following command: D:\\sites\\mydjangoapp\u003e python winservice_install It will create two files in the root directory of your project .service.py will help you install, run and remove the Windows Service. It‚Äôs much like manage.py for the service. service.ini contains the list of management commands that will be run by the Windows Service. ","date":"04-07","objectID":"/posts/2012/07/04/django-on-windows-run-celery-as-a-windows-service/:2:0","tags":["old","obsolete","django","celery","windows"],"title":"Django on Windows: Run Celery as a Windows Service","uri":"/posts/2012/07/04/django-on-windows-run-celery-as-a-windows-service/"},{"categories":["DevOps","Django"],"content":"Configuring the service A look at the service.ini file gives us the following: [services] # Services to be run on all machines run=celeryd clean=d:\\logs\\celery.log [BEATSERVER] # There should be only one machine with the celerybeat service run=celeryd celerybeat clean=d:\\logs\\celerybeat.pid;d:\\logs\\beat.log;d:\\logs\\celery.log [celeryd] command=celeryd parameters=-f d:\\logs\\celery.log -l info [celerybeat] command=celerybeat parameters=-f d:\\logs\\beat.log -l info --pidfile=d:\\logs\\celerybeat.pid [runserver] # Runs the debug server and listen on port 8000 # This one is just an example to show that any manage command can be used command=runserver parameters=--noreload --insecure 0.0.0.0:8000 [log] filename=d:\\logs\\service.log level=INFO The services section contains : The list of background commands to run in the run directive. The list of files to delete when refreshed or stopped in the clean directive. Here the run directive contains only one command: celeryd. If we look at the corresponding section of the ini file, we find: [celeryd] command=celeryd parameters=-f d:\\logs\\celery.log -l info command specifies the manage.py command to run and parameters specifies the parameters to the command. So here the configurations tells us that the service, when started, will run a python process equivalent to the command line: D:\\sites\\mydjangoapp\u003e python manage.py celeryd -f d:\\logs\\celery.log -l info And that the d:\\logs\\celery.log will be deleted between runs. The log sections defines a log file and logging level for the service process itself: [log] filename=d:\\logs\\service.log level=INFO ","date":"04-07","objectID":"/posts/2012/07/04/django-on-windows-run-celery-as-a-windows-service/:3:0","tags":["old","obsolete","django","celery","windows"],"title":"Django on Windows: Run Celery as a Windows Service","uri":"/posts/2012/07/04/django-on-windows-run-celery-as-a-windows-service/"},{"categories":["DevOps","Django"],"content":"Installing and removing the service You need to have administrator privileges to install the service in the Windows Registry so that it‚Äôs started each time the machine boots. You do that with the following command: D:\\sites\\mydjangoapp\u003e python service.py --startup=auto install The --startup=auto parameter will allow the service to start automatically when the server boots. You can check it has been installed: It can be removed with the following commands: D:\\sites\\mydjangoapp\u003e python service.py remove Please ensure that the Server Manager is not running when you run this command, because in that case a complete removal of the service will need a server restart. ","date":"04-07","objectID":"/posts/2012/07/04/django-on-windows-run-celery-as-a-windows-service/:4:0","tags":["old","obsolete","django","celery","windows"],"title":"Django on Windows: Run Celery as a Windows Service","uri":"/posts/2012/07/04/django-on-windows-run-celery-as-a-windows-service/"},{"categories":["DevOps","Django"],"content":"Starting and stopping the service The service can be manually started and stopped with the following commands: D:\\sites\\mydjangoapp\u003e python service.py start D:\\sites\\mydjangoapp\u003e python service.py stop If everything went fine, the python processes should be there: Along with the log files : ","date":"04-07","objectID":"/posts/2012/07/04/django-on-windows-run-celery-as-a-windows-service/:5:0","tags":["old","obsolete","django","celery","windows"],"title":"Django on Windows: Run Celery as a Windows Service","uri":"/posts/2012/07/04/django-on-windows-run-celery-as-a-windows-service/"},{"categories":["DevOps","Django"],"content":"Running the Beat service If you deploy your Django project on several servers, you probably want to have Celery worker processes on each deployed machine but only one unique Beat process for executing scheduled tasks. You can customize the services section of the service.ini configuration file on that specific machine, but this is incovenient if you are sharing files between machines, for instance. The service provides the ability to have several services sections in the same configuration file for different host servers. The Windows Service will try to find the section which name matches the name of the current server and will fallback to the services section if it does not find it. This allows you to have a different behaviour for the service on different machines. In the preceding configuration, you have one section, named BEATSERVER : [BEATSERVER] # There should be only one machine with the celerybeat service run=celeryd celerybeat clean=d:\\logs\\celerybeat.pid;d:\\logs\\beat.log;d:\\logs\\celery.log which adds the celerybeat command to the celeryd command. With this configuration file, the service run on a machine named BEATSERVER will run the Celery beat service. The winservice_install facility provides a convenient option for choosing the current machine as the Beat machine. Let‚Äôs try that : The new service.py file will contain a section with the name of the current machine: [WS2008R2X64] # There should be only one machine with the celerybeat service run=celeryd celerybeat clean=d:\\logs\\celerybeat.pid;d:\\logs\\beat.log;d:\\logs\\celery.log Now, when run, the service will start a new python process: And new log files for the beat service will be present: ","date":"04-07","objectID":"/posts/2012/07/04/django-on-windows-run-celery-as-a-windows-service/:6:0","tags":["old","obsolete","django","celery","windows"],"title":"Django on Windows: Run Celery as a Windows Service","uri":"/posts/2012/07/04/django-on-windows-run-celery-as-a-windows-service/"},{"categories":["DevOps","Django"],"content":"Changes to the configuration The Windows Service monitor changes to the service.ini configuration file. In case it is modified, the service does the following: Stop the background processes. Reread the configuration file. Start the background processes. You may have seen in the service.ini file the runserver section: [runserver] # Runs the debug server and listen on port 8000 # This one is just an example to show that any manage command can be used command=runserver parameters=--noreload --insecure 0.0.0.0:8000 It allows running the runserver command in a separate process. I you edit the service.ini file and add runserver to the run directive: [WS2008R2X64] # There should be only one machine with the celerybeat service run=celeryd celerybeat runserver ... As soon as you save the file, you can make your browser point to http://localhost:8000 and will obtain: ","date":"04-07","objectID":"/posts/2012/07/04/django-on-windows-run-celery-as-a-windows-service/:7:0","tags":["old","obsolete","django","celery","windows"],"title":"Django on Windows: Run Celery as a Windows Service","uri":"/posts/2012/07/04/django-on-windows-run-celery-as-a-windows-service/"},{"categories":["DevOps","Django"],"content":"Running arbitrary commands As shown in the preceding section, virtually any Django management command can be run by the service at startup or each time the service.ini file is modified. You could imagine having a section: [collectstatic] command=collectstatic parameters=--noinput ","date":"04-07","objectID":"/posts/2012/07/04/django-on-windows-run-celery-as-a-windows-service/:8:0","tags":["old","obsolete","django","celery","windows"],"title":"Django on Windows: Run Celery as a Windows Service","uri":"/posts/2012/07/04/django-on-windows-run-celery-as-a-windows-service/"},{"categories":["DevOps","Django"],"content":"Update: The configuration process described in this post can be achieved with only one management command if you install the django-windows-tools application. Windows is probably not the best production environment for Django but sometimes one doesn‚Äôt have the choice. In that case, a few options aleardy exist, most notably the one developed by helicontech that relies on Microsoft‚Äôs Web Platform Installer. This solution, which is described here, relies on the installation of a specific native Handler developed by Helicontech. This handler manages the communication between IIS and the Django application through the FastCGI protocol with the help of a little python script that bridges FastCGI to WSGI. This script is derived from the Allan Saddi flup package that is already used by Django in the manage.py runfcgi command. The flup package doesn‚Äôt work under Windows and Helicontech has made the necessary adaptations to make it work with its handler. Since its version 7, IIS does however support FastCGI natively, so the use of a specific handler to support Django is not needed. This post describes how to configure and run a Django application with the native FastCGI IIS handler. For that, I have myself adapted the Helicontech FastCGI to WSGI script to make it a Django management command. ","date":"27-06","objectID":"/posts/2012/06/27/running-django-under-windows-with-iis-using-fcgi/:0:0","tags":["old","obsolete","django","iis","windows","fcgi"],"title":"Running Django Under Windows With Iis Using Fcgi","uri":"/posts/2012/06/27/running-django-under-windows-with-iis-using-fcgi/"},{"categories":["DevOps","Django"],"content":"Python installation But before that, to run Django you will need to have python on your server. If like me for some reason it is uneasy for you to run a software installer on your server, a good choice is to use Portable Python. With it, you can install and configure your python environment on your development or staging server and install it in your production server(s) by just copying over the python folder. You can even have different python environments with different configurations on the same server. To use the portable python installation in copied in d:\\python from a command line window, juste type: set path=d:\\python\\app\\scripts;d:\\python\\app;%path% And then python and its commands are available from the command line: Another advantage of Portable Python is that it comes already bundled with The Python for Windows extensions (a.k.a. pywin32) and Django. ","date":"27-06","objectID":"/posts/2012/06/27/running-django-under-windows-with-iis-using-fcgi/:1:0","tags":["old","obsolete","django","iis","windows","fcgi"],"title":"Running Django Under Windows With Iis Using Fcgi","uri":"/posts/2012/06/27/running-django-under-windows-with-iis-using-fcgi/"},{"categories":["DevOps","Django"],"content":"Adding FastCGI to the project In our example, the Django project will be named esplayer and will be installed in d:\\sites\\esplayer. Please note that this configuration has been tested on Windows 2008 Server R2. Take the fcgi.py file and copy it in the management\\commands directory of one of your project applications so that the manage.py help fcgi command returns you: ","date":"27-06","objectID":"/posts/2012/06/27/running-django-under-windows-with-iis-using-fcgi/:2:0","tags":["old","obsolete","django","iis","windows","fcgi"],"title":"Running Django Under Windows With Iis Using Fcgi","uri":"/posts/2012/06/27/running-django-under-windows-with-iis-using-fcgi/"},{"categories":["DevOps","Django"],"content":"Configure the FastCGI application on IIS The next step is to configure the FastCGI Application on IIS. FastCGI is available whenever you have installed the CGI feature on your IIS installation. Run the server manager and go to the IIS role and configuration. Select your website. You should see a FastCGI Settings icon: Double click on it and select the Add application action. Enter the following parameters: In Full Path, enter the path to your python executable. In Arguments, enter the command line for running our fcgi command, i.e. d:\\sites\\esplayer\\esplayer\\manage.py fcgi --pythonpath=d:\\sites\\esplayer --settings=esplayer.settings. The pythonpath and settings arguments are needed to be path independent (more on this later). The other arguments are optional but you should review them to enter sensible values. The Monitor changes to file setting is particularly interesting. It will allow you to specify the path of a file that will trigger a restart of the application whenever it is modified. You can enter the path to the settings.py of your project. I personally prefer to specify a file that I explicitly update via a touch command. ","date":"27-06","objectID":"/posts/2012/06/27/running-django-under-windows-with-iis-using-fcgi/:3:0","tags":["old","obsolete","django","iis","windows","fcgi"],"title":"Running Django Under Windows With Iis Using Fcgi","uri":"/posts/2012/06/27/running-django-under-windows-with-iis-using-fcgi/"},{"categories":["DevOps","Django"],"content":"Create the website and configure it to use the FastCGI application Once we have our FastCGI application configured, we need a web site to make use of it. For it, we create a website pointing to our Django project: To make the website use our FastCGI application, we create the following web.config file in the root of our project (here d:\\sites\\esplayer\\esplayer): \u003c?xml version=\"1.0\" encoding=\"UTF-8\"?\u003e \u003cconfiguration\u003e \u003csystem.webServer\u003e \u003chandlers\u003e \u003cclear/\u003e \u003cadd name=\"FastCGI\" path=\"*\" verb=\"*\" modules=\"FastCgiModule\" scriptProcessor=\"D:\\python\\App\\python.exe|d:\\sites\\esplayer\\esplayer\\manage.py fcgi --pythonpath=d:\\sites\\esplayer --settings=esplayer.settings\" resourceType=\"Unspecified\" requireAccess=\"Script\" /\u003e \u003c/handlers\u003e \u003c/system.webServer\u003e \u003c/configuration\u003e We first clear all the request handlers and then specify that every request (path=\"*\" and verb=\"*\") should be managed by the FastCgiModule module. The scriptProcessor attribute reproduces the Full Path and the Arguments of our FastCGI application separated by |. It allows the module to identify the FastCGI application to which the requests will be routed. ","date":"27-06","objectID":"/posts/2012/06/27/running-django-under-windows-with-iis-using-fcgi/:4:0","tags":["old","obsolete","django","iis","windows","fcgi"],"title":"Running Django Under Windows With Iis Using Fcgi","uri":"/posts/2012/06/27/running-django-under-windows-with-iis-using-fcgi/"},{"categories":["DevOps","Django"],"content":"Static files With the preceding web.config configuration, all the requests are routed to the Django application. However, we want the static files of our application to be managed by IIS itself. To do that, we first configure Django to collect the static files in the static subdirectory of our project. For that, we have the following configuration in our settings.py file: SITE_ROOT = os.path.abspath(os.path.dirname(__file__)) ... STATIC_URL = '/static/' ... STATIC_ROOT = os.path.join( SITE_ROOT, 'static') SITE_STATIC_ROOT = os.path.join( SITE_ROOT, 'local_static') # Additional locations of static files STATICFILES_DIRS = ( # Don't forget to use absolute paths, not relative paths. ('', SITE_STATIC_ROOT), ) .. The project wide defined static files are located in the local_static directory. All the static files are collected in the static directory by running the following command: python manage.py collecstatic In the local_static directory we put the following web.config file: \u003c?xml version=\"1.0\" encoding=\"UTF-8\"?\u003e \u003cconfiguration\u003e \u003csystem.webServer\u003e \u003c!-- this configuration overrides the FastCGI handler to let IIS serve the static files --\u003e \u003chandlers\u003e \u003cclear/\u003e \u003cadd name=\"StaticFile\" path=\"*\" verb=\"*\" modules=\"StaticFileModule\" resourceType=\"File\" requireAccess=\"Read\" /\u003e \u003c/handlers\u003e \u003c/system.webServer\u003e \u003c/configuration\u003e Which basically inverts the web.config file or the root of the project by clearing all the handlers and serving all requests only as static files. When collected, this file will go in the static directory and will instruct IIS that all requests below the path /static should be served as static files. ","date":"27-06","objectID":"/posts/2012/06/27/running-django-under-windows-with-iis-using-fcgi/:5:0","tags":["old","obsolete","django","iis","windows","fcgi"],"title":"Running Django Under Windows With Iis Using Fcgi","uri":"/posts/2012/06/27/running-django-under-windows-with-iis-using-fcgi/"},{"categories":["DevOps","Django"],"content":"Website creation automation The website creation that is described in the previous sections can be automated with the following script that must be run as an administrator: %windir%\\system32\\inetsrv\\appcmd.exe set config -section:system.webServer/fastCgi /+\"[fullPath='d:\\python\\app\\python.exe',arguments='d:\\sites\\esplayer\\esplayer\\manage.py fcgi --pythonpath=d:\\sites\\esplayer --settings=esplayer.settings',maxInstances='4',idleTimeout='1800',activityTimeout='30',requestTimeout='90',instanceMaxRequests='100000',protocol='NamedPipe',flushNamedPipe='False',monitorChangesTo='d:\\sites\\esplayer\\esplayer\\web.config']\" /commit:apphost %windir%\\system32\\inetsrv\\appcmd.exe add apppool /name:esplayer %windir%\\system32\\inetsrv\\appcmd.exe add site /name:esplayer /bindings:http://*:80 /physicalPath:d:\\sites\\esplayer\\esplayer %windir%\\system32\\inetsrv\\appcmd.exe set app \"esplayer/\" /applicationPool:esplayer The four commands run in the script do the following actions: Create the FastCGI application. Create the site application pool. Create the website. Add the created website to the application pool. ","date":"27-06","objectID":"/posts/2012/06/27/running-django-under-windows-with-iis-using-fcgi/:6:0","tags":["old","obsolete","django","iis","windows","fcgi"],"title":"Running Django Under Windows With Iis Using Fcgi","uri":"/posts/2012/06/27/running-django-under-windows-with-iis-using-fcgi/"},{"categories":["DevOps","Django"],"content":"Testing and troubleshooting After the configuration, the website should be available through IIS. If this is not the case, you will probably get a 500 Error: The first thing to do is to check that the website is available outside of IIS by running it with the command: python manage.py runserver 0.0.0.0:8000 And accessing it on http://localhost:8000. If the application works as a standalone Django application, the most common cause of error is a misconfiguration of either the FastCGI application or the root web.config file. You need to be sure that the The scriptProcessor attribute of the web.config matches Full Path and the Arguments of the FastCGI application. To troubleshoot further, the fcgi.py command provides several settings to be put in the settings.py file : FCGI_LOG (default False), when True, instructs the command to create a log file in the path pointed by FCGI_LOG_PATH. If FCGI_LOG_PATH is not defined, the log file will be created in the project root directory. The file name name pattern of the log file will be fcgi_AAMMDD_HHMMSS_XXXX.log, in which AAMMDD is the date, HHMMSS the time and XXXX the FastCGI application process number. If DEBUG is set to True in the settings, the log file will contain the Django debug logs. The FCGI_DEBUG setting (default False), when set to True, will output in the log file information about the FCGI protocol transfers between IIS and the Django application. ","date":"27-06","objectID":"/posts/2012/06/27/running-django-under-windows-with-iis-using-fcgi/:7:0","tags":["old","obsolete","django","iis","windows","fcgi"],"title":"Running Django Under Windows With Iis Using Fcgi","uri":"/posts/2012/06/27/running-django-under-windows-with-iis-using-fcgi/"},{"categories":["DevOps","Django"],"content":"Easing the FastCGI configuration It is somewhat painful to have to specify the pythonpath and settings parameters both in the FastCGI configuration and in the web.config file. To avoid entering them each time, I have created a manage.py script in the scripts subdirectory of the project root that auto configures itself. Here is the source of the file: #!/usr/bin/python # -*- coding: utf-8 -*- import os,sys from os.path import abspath, dirname # the base path is my parent directory base_path = dirname(dirname(abspath(__file__))) from django.core.handlers.modpython import handler # Add the parent directory to the path to be able to import settings sys.path.append(base_path) sys.path.append(dirname(base_path)) # Now we can import our settings and setup the environment try: import settings # Assumed to be in the same directory. except ImportError: import sys sys.stderr.write(\"Error: Can't find the file 'settings.py' in the directory containing %r. It appears you've customized things.\\nYou'll have to run django-admin.py, passing it your settings module.\\n(If the file settings.py does indeed exist, it's causing an ImportError somehow.)\\n\" % __file__) sys.exit(1) from django.core.management import setup_environ setup_environ(settings) from django.core.management import execute_manager if __name__ == \"__main__\": execute_manager(settings) With this script, the Arguments setting of the FastCGI application becomes d:\\sites\\esplayer\\esplayer\\scripts\\manage.py fcgi and the scriptProcessor attribute in the web.config file becomes scriptProcessor=\"D:\\python\\App\\python.exe|d:\\sites\\esplayer\\esplayer\\script\\manage.py fcgi\" ","date":"27-06","objectID":"/posts/2012/06/27/running-django-under-windows-with-iis-using-fcgi/:8:0","tags":["old","obsolete","django","iis","windows","fcgi"],"title":"Running Django Under Windows With Iis Using Fcgi","uri":"/posts/2012/06/27/running-django-under-windows-with-iis-using-fcgi/"},{"categories":["DevOps","Django"],"content":"What next Once this configuration is done on a project and a server, replicating it across multiple servers is easy as the only configuration not part of the project is the one of the FastCGI application. Most configuration files are ported from server to server with the source code of the project. However, the first creation and configuration could benefit from having some management commands dedicated to it. These would be part, along with the fcgi.py command, of a specific Django application that could be added to any project. Furthermore, some of you may have noted that having the website point to the root of the Django project is not mandatory. Thus the Django project itself could be part of the python installation itself and deployed by running a Django management command. ","date":"27-06","objectID":"/posts/2012/06/27/running-django-under-windows-with-iis-using-fcgi/:9:0","tags":["old","obsolete","django","iis","windows","fcgi"],"title":"Running Django Under Windows With Iis Using Fcgi","uri":"/posts/2012/06/27/running-django-under-windows-with-iis-using-fcgi/"},{"categories":["Development","Android"],"content":"Adding logs to your Android source code is sometimes the only way to really understand what happens , especially in asynchronous situations. If you are lazy like me, you may insert lazy logs like this one: Log.v(\"#LOOK#\", \"onStart()\"); Instead of having less lazy code like: public class SomeActivity extends Activity { private static final String LOG_TAG = SomeActivity.class .getSimpleName(); private static final int LOG_LEVEL = Log.VERBOSE; ... @Override public void onStart() { if (LOG_LEVEL \u003c= Log.VERBOSE) Log.v(LOG_TAG, \"onStart()\"); But Eclipse can easily help you to avoid this and then the need to clean up your code after debbuging. Everybody uses content assist in Eclipse. The CTRL+Space shortcut alleviates us from the need to type all those long field and method names that come out of our imagination. With the Templates feature, it can even write code for us. Templates are editable in the preferences. To see them, select Window \u003e Preferences and then in the preferences dialog, Java \u003e Editor \u003e Templates. The window looks like this: If you double click on a template you can edit it: The template name is what you type in the Editor window before hitting CTRL+Space and that will make Eclipse propose you the template. I won‚Äôt go into a full explanation of the syntax of the templates, but basically the template name is replaced by the template pattern and the content between the ${} is replaced either by what you type or by values computed by existing macros. DZone gives you a good Visual Guide to Template listing most common macros. Now we can create our templates for both adding the Log declarations at the beginning of our class as well as templates for inserting conditionally ran logs. For the header, we create a template named alh in Java types member context with the following pattern: ${:import(android.util.Log)}private static final String LOG_TAG = ${enclosing_type}.class.getSimpleName(); private static final int LOG_LEVEL = Log.${level}; ${cursor} The different ${} mean: ${:import(android.util.Log)}: make sure android.util.Log is imported. ${enclosing_type}: insert the name of the type (class) we‚Äôre in. ${level}: when inserting, put the cursor here and wait for the user to enter the level variable. ${cursor}: leave the cursor here when the user hits the ENTER key. With this template, inserting the log headers in a class is achievied with the following steps: type alh and hit STRL+Space. select the template (first choice) and hit ENTER enter the desired log level (DEBUG for instance) for the class and hit ENTER. continue coding. This is much simpler than copy-pasting the code from another class and replacing the class name and log level. The following template, named alv in the Java statement context is for inserting verbose logs: if (LOG_LEVEL \u003c= Log.VERBOSE) Log.v(LOG_TAG, \"${enclosing_method}() ${}\"); ${cursor} The nice thing is that it inserts the name of the current method and wait just after for your debug message. Just typing Enter will leave a log like: if (LOG_LEVEL \u003c= Log.VERBOSE) Log.v(LOG_TAG, \"onStart() \"); Which may be just enough. On this model, you can create ali, ald, ale templates for the different debug levels, or if you want to use String.format() templates like : if (LOG_LEVEL \u003c= Log.DEBUG) Log.d(LOG_TAG, String.format(\"${enclosing_method}() ${}\", ${args})); ${cursor} Just adapt them to your needs. Once you have finished debugging, if you change the LOG_LEVEL of your class from let‚Äôs say VERBOSE to INFO, all the alv templates you‚Äôve entered will become dead code as the if surrounding the log lines is always false. This is because it compares static variables, and this is just what we want. When we compile for delivery, we want the compiler to optimize out all this code from the binary. However, the Java compiler will generate warnings for that. As it is not possible to surround the log with a @SuppressWarnings() attribute, you may want to change the error level of dead code from Warning to Ignore. T","date":"24-03","objectID":"/posts/2012/03/24/using-eclipse-templates-to-ease-android-logging/:0:0","tags":["old","obsolete","android","eclipse"],"title":"Using Eclipse Templates to Ease Android Logging","uri":"/posts/2012/03/24/using-eclipse-templates-to-ease-android-logging/"},{"categories":["Android"],"content":"Sometimes in Android, the flexible layout system is not flexible enough and you need to make some computations inside your code. In these computations, you may need to subtract the size of the status bar. Stackoverflow gives you some answers, but they all rely on the fact that te status bar is shown at the time you make your computation. If you are in full screen mode, by having called for instance: getWindow().setFlags(WindowManager.LayoutParams.FLAG_FULLSCREEN, WindowManager.LayoutParams.FLAG_FULLSCREEN) It doesn‚Äôt work. The height of the status bar is contained in a dimension resource called status_bar_height. It‚Äôs not part of the public resources, so you can‚Äôt access it directly from your code with android.R.dimen.status_bar_height. You can however compute it at runtime with the following code: public int getStatusBarHeight() { int result = 0; int resourceId = getResources().getIdentifier(\"status_bar_height\", \"dimen\", \"android\"); if (resourceId \u003e 0) { result = getResources().getDimensionPixelSize(resourceId); } return result; } You need to put this method in a ContextWrapper class. Hope it helps. ","date":"17-03","objectID":"/posts/2012/03/17/get-the-height-of-the-status-bar-in-android/:0:0","tags":["old","android"],"title":"Get the Height of the Status Bar in Android","uri":"/posts/2012/03/17/get-the-height-of-the-status-bar-in-android/"},{"categories":["Development","Django"],"content":"If you are tired to fire a terminal window, cd to your project directory and activate your python virtualenv to get to your Django project, you will find here some tips to improve things a little bit. This tip is divided in two parts : First we create a shell startup script that activates the virtualenv, bash completion and cd in the project directory. Then we create a Linux Desktop Entry file That spawns a console in our environment. Here you have the startup script: #!/bin/bash # # The layout of the development environment is assumed to be: # # \u003cpyton virtual env\u003e/ # src/ # \u003cproject name\u003e/ # .consolerc (this file) # setup.py # ... # \u003cproject name\u003e/ # manage.py # settings.py # ... # # Run the standard bash rc file source ~/.bashrc # Get the current source file name current=\"${BASH_SOURCE[0]}\" # Retrieve the source directory DJANGO_SOURCE_DIR=\"$(dirname \"$(readlink -f \"$current\")\")\" # Get the Django related directories DJANGO_PROJECT_NAME=\"$(basename \"$DJANGO_SOURCE_DIR\")\" DJANGO_ENV_DIR=$(readlink -f \"${DJANGO_SOURCE_DIR}/../../\") DJANGO_PROJECT_DIR=\"${DJANGO_SOURCE_DIR}/${DJANGO_PROJECT_NAME}\" # Activate the environment source \"${DJANGO_ENV_DIR}/bin/activate\" cd \"$DJANGO_PROJECT_DIR\" export PATH=\"$PATH:$(pwd)\" # Retrieve the Django bash completion file (only once) and execute it. # This is potentially insecure. DJANGO_BASH_COMPLETION=\"${DJANGO_SOURCE_DIR}/.django_bash_completion\" if [ ! -f \"$DJANGO_BASH_COMPLETION\" ]; then curl http://code.djangoproject.com/svn/django/trunk/extras/django_bash_completion -o \"$DJANGO_BASH_COMPLETION\" 2\u003e/dev/null fi source \"$DJANGO_BASH_COMPLETION\" # Miscellaneous alias runserver='cd $DJANGO_PROJECT_DIR;manage.py runserver 0.0.0.0:8000' The comment at the beginning explains how the project directory layout is assumed to be. That is the only assumption that makes the script. In consequence, it is reusable as is in any other project. Here is the .desktop file that runs a terminal console with our script: [Desktop Entry] Exec=/bin/bash --rcfile .consolerc GenericName[fr]=MyProject Django GenericName=MyProject Django Icon=/home/antoine/images/django-icon_0.png MimeType= Name[fr]=MyProject Django Name=MyProject Django Path=/home/antoine/src/django/my_project/src/my_project/ StartupNotify=true Terminal=true TerminalOptions= Type=Application Categories=Development The command runs in a terminal because of Terminal=true. You can see that apart from Name and GenericName, the only line specific to the project is Path=/home/antoine/src/django/my_project/src/my_project/ It defines the project path, making it easy to reuse. The execution of our init script is done through: Exec=/bin/bash --rcfile .consolerc The Icon is the familiar Django icon : I personally put the .desktop file in $HOME/Desktop, but it also can reside in $HOME/.local/share/applications. In that case, the entry will be available in the menu. I‚Äôve tested this under KDE, but it should work also with Gnome. ","date":"13-03","objectID":"/posts/2012/03/13/start-a-virtualenv-django-shell-from-the-linux-desktop/:0:0","tags":["old","obsolete","snippet","django","linux","bash"],"title":"Start a Virtualenv Django Shell From the Linux Desktop","uri":"/posts/2012/03/13/start-a-virtualenv-django-shell-from-the-linux-desktop/"},{"categories":["Development","Django","Android"],"content":"Google play, formerly known as the Android Market, provides in-app billing in several countries. In the Security and Design page, Google states the following: If practical, you should perform signature verification on a remote server and not on a device. Implementing the verification process on a server makes it difficult for attackers to break the verification process by reverse engineering your .apk file. If you do offload security processing to a remote server, be sure that the device-server handshake is secure. The signature verification here refers to the signature sent back by the Billing Service to the GET_PURCHASE_INFORMATIONrequest. The signature is against the JSON payload containing the purchase information. We‚Äôllget back later on the authentication of the dialog with the server. The JSON payload looks like the following (It has been indented for readability): { \"nonce\":7822246098812800204, \"orders\":[ { \"notificationId\":\"-915368186294557970\", \"orderId\":\"971056902421676\", \"packageName\":\"com.xxx.yyy\", \"productId\":\"com.xxx.yyy.product\", \"purchaseTime\":1331562686000, \"purchaseState\":1, \"developerPayload\":\"WEHJSU\" } ] } And we we receive a signature in Base64: rKf9B38gLbJaLiyRbQVJNr0i0IvJxBgi3EmsLoZLkFedZvn642s4+fz3jYCk6IVWWFSqtBH2Z8ChONJkHWrkDUCK79uSBPLN5s4x4AsRHgQ8aw3sRQLAoEDMFA1ym1gkfYfDz+6sxP2Rgg1U/qpHIEHWPDbJAdP7zcM1iz2kEWbYvFwKP3NNWExNB4gWH3IFtPR0l/KLjKBoqpX5zVukmUeaZW0Skx10eFROa4VhqA5JrbZZQwK0jc6FCYi3u6c1ryIw6W5tcdIv1PFOKpE7VMq67yyD+IEXc+nl29FN5ByGhkj/khNY1KLXcszCCa7ygSYw7mQI+omLdyMz6aL3hg== The payload is signed with the Private key associated with you Google Play account. You can grab your public key in your developer console page. There are several crypto solutions available in python. In our example, we use pycrypto. It can easily be installed in your Django virtual environment with: \u003e pip install pycrypto Then, the following method allows checking of the payload signature: from Crypto.Signature import PKCS1_v1_5 from Crypto.Hash import SHA from Crypto.PublicKey import RSA import base64 PUBLIC_KEY='\u003cPut here your public key\u003e' VERIFY_KEY= RSA.importKey(base64.decodestring(PUBLIC_KEY)) def verify_signature(message, signature): '''Verify that signature is the result of signing message''' # Get the hash of the message h = SHA.new(message) # Create a verifier verifier = PKCS1_v1_5.new(VERIFY_KEY) # decode the signature signature = base64.decodestring(signature) # verify return verifier.verify(h, signature) In a next post, we‚Äôll se how to make sure on the Android application side that the responses to our requests are really coming from our server. ","date":"12-03","objectID":"/posts/2012/03/12/checking-google-play-signatures-with-django/:0:0","tags":["old","android","django","android","security"],"title":"Checking Google Play Signatures With Django","uri":"/posts/2012/03/12/checking-google-play-signatures-with-django/"}]